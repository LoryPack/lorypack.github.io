<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://lorenzopacchiardi.me//feed.xml" rel="self" type="application/atom+xml"/><link href="http://lorenzopacchiardi.me//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-27T16:35:30+00:00</updated><id>http://lorenzopacchiardi.me//feed.xml</id><title type="html">blank</title><subtitle>Lorenzo Pacchiardi, Research Associate at the Center for the Future of Intelligence, University of Cambridge. </subtitle><entry><title type="html">2024 May “AI Evaluation” Digest</title><link href="http://lorenzopacchiardi.me//blog/2024/ai-evaluation/" rel="alternate" type="text/html" title="2024 May “AI Evaluation” Digest"/><published>2024-05-31T17:39:00+00:00</published><updated>2024-05-31T17:39:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2024/ai-evaluation</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2024/ai-evaluation/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I contributed to the May edition of the "AI Evaluation" digest (on substack)]]></summary></entry><entry><title type="html">2024 April “AI Evaluation” Digest</title><link href="http://lorenzopacchiardi.me//blog/2024/ai-evaluation/" rel="alternate" type="text/html" title="2024 April “AI Evaluation” Digest"/><published>2024-04-26T17:39:00+00:00</published><updated>2024-04-26T17:39:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2024/ai-evaluation</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2024/ai-evaluation/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I contributed to the April edition of the "AI Evaluation" digest (on substack)]]></summary></entry><entry><title type="html">2024 January “AI Evaluation” Digest</title><link href="http://lorenzopacchiardi.me//blog/2024/ai-evaluation/" rel="alternate" type="text/html" title="2024 January “AI Evaluation” Digest"/><published>2024-01-27T17:39:00+00:00</published><updated>2024-01-27T17:39:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2024/ai-evaluation</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2024/ai-evaluation/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I contributed to the January edition of the "AI Evaluation" digest (on substack)]]></summary></entry><entry><title type="html">Finetuning GPT3</title><link href="http://lorenzopacchiardi.me//blog/2022/GPT3_finetuning/" rel="alternate" type="text/html" title="Finetuning GPT3"/><published>2022-11-26T15:09:00+00:00</published><updated>2022-11-26T15:09:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2022/GPT3_finetuning</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2022/GPT3_finetuning/"><![CDATA[<p>Large Language Models have quite extraordinary performance. They can generate text, translate languages, and even answer questions. However, they are not perfect (fun fact: this last sentence was actually suggested by Github copilot, which is powered by a language model :laughing:). If you have a specific task, you may want to finetune the model on your own dataset.</p> <p>OpenAI API allows you to do that. You can finetune GPT3 and then using it as you wish. In the notebook below, I show how to finetune GPT3 to predict the title of an arxiv paper from its abstract. Hope you enjoy it!</p> <p><a href="https://colab.research.google.com/gist/LoryPack/c53b41e7041f77ac450f7394455d2d2e#scrollTo=GwQeLZWFGs7_"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p> <p>One disclaimer: to use the OpenAI API you need to register and using it has a monetary cost.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Using the OpenAI API to finetune GPT3 on a custom dataset]]></summary></entry><entry><title type="html">Generalizing Bayesian Inference</title><link href="http://lorenzopacchiardi.me//blog/2021/generalizedBayes/" rel="alternate" type="text/html" title="Generalizing Bayesian Inference"/><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2021/generalizedBayes</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2021/generalizedBayes/"><![CDATA[<p>If you are reading this, you probably know what Bayes’ theorem is. Here, we are concerned with the use of Bayes’ theorem to perform inference for parameters of a statistical model (a.k.a. <strong>Bayesian inference</strong>).</p> <p>Recently, several generalizations of Bayesian inference have been proposed. The aim of this post is to survey some of these generalizations and overview the pros and cons of each with respect to using the original Bayes theorem. Hopefully, I’ll be able to put some order in this rapidly expanding literature and provide some intuition on why we need to move beyond the original Bayes theorem.</p> <p><strong>TL; DR:</strong> Extensions of Bayesian inference work better in some cases which are not contemplated by using the original Bayes’ theorem (primarily, model misspecification), and provide more justification.</p> <h1 id="the-original-bayes-theorem">The original Bayes theorem</h1> <div style="text-align:center;font-size:15px;"> <div class="l-body"> <img class="img-fluid" src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif"/> </div> <i> A portrait of Thomas Bayes (maybe). Source: <a href="https://commons.wikimedia.org/wiki/File:Thomas_Bayes.gif">Unknown author</a>, Public domain, via Wikimedia Commons.</i> </div> <p><br/> Let’s first have a look at Bayes’ original theorem <d-footnote>This is actually the form that was given to it by Laplace; click <a href="https://www.lesswrong.com/posts/RTt59BtFLqQbsSiqd/a-history-of-bayes-theorem" target="_blank"> here </a> for a nice history of the theorem. </d-footnote>:</p> \[\pi(\theta\vert x) = \frac{\pi(\theta) p(x\vert \theta)}{p(x)}, \qquad p(x) = \int \pi(\theta) p(x\vert \theta) d\theta.\] <p>Here, I am using \(\theta\) to denote a <em>parameter</em> on which we want to perform inference, while \(x\) is the <em>data</em> that we observe.</p> <p>As the standard textbook description of Bayesian inference says, Bayes’ theorem provides a <em>posterior belief</em> \(\pi(\theta\vert x)\), by updating the <em>prior belief</em> \(\pi(\theta)\) with the information on \(\theta\) carried by \(x\). Crucially, all information on \(\theta\) contained in \(x\) is represented by the <em>likelihood</em> \(p(x\vert \theta)\), such likelihood being the probabilistic model from which <strong>we believe</strong> \(x\) was generated. This is the gist of Bayesian inference.</p> <p>Actually, the denominator in the definition of the posterior is independent on \(\theta\). Exploiting this fact, it is common to write:</p> \[\pi(\theta\vert x) \propto \pi(\theta) p(x\vert \theta),\] <p>where the proportional sign refers to both sides of the equation being considered as function of \(\theta\)<d-footnote>When some posterior expectations need to be computed, the function on the right-hand side needs to be normalized (ie divided by the normalizing constant $p(x)$). However, computing the normalizing constant is not needed when sampling from the posterior with MCMC methods, for instance.</d-footnote>.</p> <p>Additionally, Bayes’ theorem is very modular, allowing to sequentially incorporate new information into our belief: say that we previously had data \(x_1\), from which we obtained \(\pi(\theta\vert x_1)\). If we now get an independent observation \(x_2\), we can apply Bayes’ theorem again by using \(\pi(\theta\vert x_1)\) as a prior and get:</p> \[\pi(\theta\vert x_1, x_2) \propto \pi(\theta\vert x_1) p(x_2\vert \theta).\] <p>So, if we get \(n\) independent observations, say \(\mathbf {x} = (x_1, x_2, \ldots, x_n)\), the posterior belief is:</p> \[\pi(\theta\vert \mathbf x) = \frac{\pi(\theta) \prod_{i=1}^n p(x_i\vert \theta)}{p(\mathbf x)} \propto \pi(\theta) \prod_{i=1}^n p(x_i\vert \theta).\] <h2 id="properties-of-bayes-posterior">Properties of Bayes posterior</h2> <p>The posterior obtained from Bayes’ theorem satisfies some nice properties; first, there are properties that are intrinsic to Bayes’ update rule:</p> <ul> <li><strong>coherence:</strong> in whatever order you process the observations \(x_i\), you end up with the same posterior belief<d-footnote>This property may be given different names in other works, as for instance <i>Bayesian additivity</i>.</d-footnote>.</li> <li><strong>Likelihood principle:</strong> As the observations \(\mathbf x\) only appear through the likelihood in the definition of the posterior, Bayes’ theorem satisfies the likelihood principle, which says that (from <a href="https://en.wikipedia.org/wiki/Likelihood_principle">Wikipedia</a>): <blockquote> <p>given a statistical model, all the evidence in a sample relevant to model parameters is contained in the likelihood function.</p> </blockquote> </li> </ul> <p>If you further assumes that the observations \(\mathbf x\) are generated from \(p(\cdot\vert\theta_0)\) for some parameter value \(\theta_0\) (i.e., the model is <em>well specified</em>), Bayes’ posterior satisfies some additional properties:</p> <ul> <li> <p>according to information theory arguments in Zellner <d-cite key="zellner1988optimal"></d-cite>, Bayes theorem is the <strong>optimal way to process information</strong>, in the sense that its use does not discard any information present in the data about the parameters.</p> </li> <li> <p>An analogue to the Central Limit Theorem in frequentist statistics applies to Bayes’ posterior, called <strong>Bernstein-von Mises theorem</strong>. It goes like this: as the number of observations \(n\) goes to infinity, the posterior converges to a normal distribution which is centered in \(\theta_0\) (and whose variance decreases as \(1/n\) and is asymptotically equivalent to the sampling variance of the maximum likelihood estimator of \(\theta\))<d-footnote>The theorem of course holds under some regularity conditions, such as $\pi(\theta_0)&gt;0$; click <a href="https://encyclopediaofmath.org/wiki/Bernstein-von_Mises_theorem#References" target="_blank"> here </a> for an introduction and some further references. </d-footnote>. That means that Bayesian inference is, asymptotically, equivalent to maximum likelihood estimation, as it recovers the exact parameter value \(\theta_0\).</p> </li> </ul> <h2 id="so-what-is-wrong">So what is wrong?</h2> <p>To recap what we said above, the main motivation behind Bayes’ theorem is that, if the model \(p(x\vert \theta)\) is well specified, the posterior distribution is a coherent way to learn about the true parameter value (to which it converges in the limit of infinite data); additionally, that is the best possible way to process information even with a finite amount of observations (by Zellner’s argument).</p> <p>However, these arguments vacillate when the model is not an accurate representation of the distribution \(g(\cdot)\) from which observations were generated (or data-generating process, DGP). By this, we mean that there do not exist any parameter value such that \(p(\cdot\vert\theta)=g(\cdot)\). If that is the case, two things happen:</p> <ul> <li>Zellner’s argument simply does not hold anymore <d-cite key="holmes2017assigning"></d-cite>.</li> <li>A Bernstein von-Mises result still holds, but now the asymptotic normal distribution will be centered in the parameter value \(\theta^\star = \arg \min_{\theta} D_{\text{KL}}(g(\cdot)\vert \vert p(\cdot|\theta)),\) where \(D_{\text{KL}}\) is the Kullback-Leibler (KL) divergence<d-footnote> Notice that $\theta^\star$ is also the parameter value to which the frequentist maximum likelihood estimate converges</d-footnote>.</li> </ul> <p>More in general, we need to ask what is the aim of Bayesian inference in such a <strong>misspecified</strong> setting. In fact, with Bayesian inference we do not learn about the <em>true</em> parameter value anymore, as such thing does not exist. Rather, the standard Bayes’ posterior learns about the parameter value such that the misspecified model is as close as possible to the DGP in the specific sense of the KL divergence.</p> <p>That may still be what you want to do in some cases, but I argue below that using the KL divergence may behave poorly in some misspecified cases; instead, some generalized Bayesian inference allow the user to choose the way in which you approximate the DGP with the probabilistic model (for instance replacing the KL with other divergences <d-cite key="pacchiardi2021generalized"></d-cite><d-cite key="jewson2018principles"></d-cite>). Others instead completely dispose of a probabilistic model.</p> <h3 id="issues-with-the-kl-divergence">Issues with the KL divergence</h3> <p>Learning \(\theta^\star\) according to the KL divergence may not behave well when the model is misspecified; for instance, consider the following DGP:</p> \[g(x) = 0.1\ h(x) + 0.9\ p(x|\theta_0);\] <p>this means that 90% of observations are generated from the model for a given parameter value \(\theta_0\), while the other 10% are generated from the distribution \(h\). If \(h\) has heavier tails than \(p\), \(\theta^\star\) can be far away from \(\theta_0\), as the KL divergence gives large importance to the tail behavior of the distributions<d-cite key="jewson2018principles"></d-cite>. With a finite amount of samples, that translates into saying that Bayes’ posterior is highly sensitive to outliers in the data.</p> <p>With more general misspecifications, things may go wrong in different ways.</p> <h3 id="prior-and-computational-limits">Prior and computational limits</h3> <p>Finally, Bayesian inference implicitly assumes that the prior is a good representation of previous knowledge, and that enough computational power is available to sample from the posterior (which, in some cases, it is very expensive to do) <d-cite key="knoblauch2019generalized"></d-cite>.</p> <p>Some of the generalized Bayesian approaches <d-cite key="knoblauch2019generalized"></d-cite> explicitly include the fact that these assumptions may be broken in the definition of the inference strategy, as we will see below.</p> <h1 id="generalizations-of-bayes-posterior">Generalizations of Bayes posterior</h1> <p>In the following, I review extensions to Bayes theorem which are more justified and may have better performances in a misspecified setting (or can even be used without a probabilistic model); some will also tackle directly the issues regarding prior and computational power.</p> <p>Across these works, a recurrent underlying question is: what is the actual aim of inference when we cannot specify the model correctly?</p> <p><strong>Disclaimer</strong>: this overview is non-exhaustive and strongly biased due to papers I’ve read and my personal research activity.</p> <h2 id="power-likelihood">Power likelihood</h2> <p>A first idea to tackle (mild) model misspecification is to reduce the importance of the likelihood term in the definition of Bayes’ posterior. This can be done by raising the likelihood function to a power \(\alpha \in [0,1]\), where \(\alpha=1\) recovers Bayes’ posterior and \(\alpha=0\) corresponds to sticking to the prior. The resulting posterior is therefore:</p> \[\pi_\alpha(\theta\vert x) \propto \pi(\theta) p(x\vert \theta)^\alpha,\] <p>This idea has been discussed in detail in<d-cite key="holmes2017assigning"></d-cite> (and previous papers referred there). The authors of <d-cite key="holmes2017assigning"></d-cite> also propose a way to automatically tune \(\alpha\); this way works by matching the expected information gain in two experiments, one involving the exact data-generating process and the other one involving instead the best model approximation. This strategy recovers \(\alpha=1\) in the well specified case, and sets \(\alpha&lt;1\) otherwise. Other methods for tuning \(\alpha\) are reviewed in <d-cite key="wu2020comparison"></d-cite>.</p> <p>With respect to standard Bayes’ posterior, this strategy does not change the parameter value on which learning is performed, but it only changes the speed of learning (ie the rate of concentration of the posterior distribution with an increasing number of observations). Also, it still satisfies the likelihood principle and coherence.</p> <h2 id="loss-based-bayesian-inference">Loss-based Bayesian inference</h2> <p>A larger leap is taken in <d-cite key="bissiri2016general"></d-cite>. There, the authors replace the likelihood term with the exponential of a loss function:</p> <p>\begin{equation}\label{Eq:bissiri} \pi_{\ell,w}(\theta\vert x) \propto \pi(\theta) \exp{ - w \cdot \ell (\theta,x)}, \end{equation}</p> <p>where \(w\) is a weight which balances the influence of the loss and prior term. With this update, as argued in <d-cite key="bissiri2016general"></d-cite>, you learn about the parameter value minimizing the expected loss:</p> \[\int \ell (\theta,x) g(x) dx,\] <p>where \(g\) is the data-generating process.</p> <p>In <d-cite key="bissiri2016general"></d-cite>, they show how the update rule above (Eq. \eqref{Eq:bissiri}) can be derived axiomatically from the task of learning about the parameter value minimizing the expected loss, by assuming the observed data to be independent from the prior and inference to be <strong>coherent</strong> in the sense defined above (ie invariant to the order in which the observations were obtained).</p> <p>An advantage to this approach is that you do not need to specify a probabilistic model (ie likelihood) here; inference can be performed on “parameter” values that are solely defined through the loss function.</p> <p>Of course, the power likelihood posterior is obtained as a special case by setting \(\ell(\theta, x) = - \log p(x\vert \theta)\), and \(w=\alpha\); further setting \(w=1\) gives back the original posterior.</p> <p>However, with a generic loss, setting the value of \(w\) is an arbitrary choice. The larger \(w\), the faster the concentration of the posterior with the increase of the number of observations. In <d-cite key="bissiri2016general"></d-cite>, some heuristic techniques to tune \(w\) are discussed; in general, however, the choice of \(w\) is a subjective choice of the user, which may be driven by practical and computational reasons as well.</p> <p>Finally, notice that a Bernstein-von Mises result still hold for this more general posterior (of course under some regularity conditions). Some very general and applicable formulations are given in <d-cite key="miller2019asymptotic"></d-cite>.</p> <h3 id="scoring-rules-based-bayesian-inference">Scoring rules-based Bayesian inference</h3> <p>So, the loss-based approach in Eq. \eqref{Eq:bissiri} works without a probabilistic model. But what if you have a misspecified model which you believe carries some meaning about the process you are studying?</p> <p>As mentioned above, using the original Bayes’ posterior may not be a wise choice. In order to perform inference in a sounded way, an idea is to use the loss-based approach and express the loss \(\ell\) as a function of the probabilistic model:</p> \[\ell(\theta, x) = S(p(\cdot|\theta), x),\] <p>where \(S\) is a function of a probability distribution and an observation, which is usually called <em>scoring rule</em> in the literature<d-cite key="gneiting2007strictly"></d-cite>. Therefore, this yields the following update:</p> \[\pi_{S,w}(\theta\vert x) \propto \pi(\theta) \exp\{ - w \cdot S (p(\cdot\vert\theta),x)\}.\] <p>This approach has been investigated in some recent works, amongst which <d-cite key="pacchiardi2021generalized"></d-cite>, <d-cite key="jewson2018principles"></d-cite> , <d-cite key="loaiza2019focused"></d-cite> and <d-cite key="giummole2019objective"></d-cite>. In this way, you learn about the parameter value minimizing the expected scoring rule over the data generating process \(g\):</p> \[\int S (p(\cdot|\theta),x) g(x) dx.\] <p>For some scoring rules \(S\), the above expectation corresponds to a statistical divergence (for instance the energy distance and MMD <d-cite key="pacchiardi2021generalized"></d-cite>). Therefore, you can learn about the parameter value for which the model is closer to the data generating process according to different divergences from the KL one (which, as discussed at the beginning, is what the original Bayes’ posterior leads to). If you are interested for instance in prediction tasks (for which scoring rules are often used as evaluation criteria), this approach is a way to obtain better predictions with a misspecified model in a Bayesian setting<d-cite key="loaiza2019focused"></d-cite>.</p> <p>Clearly, this still gives a coherent update, but the likelihood principle is not satisfied anymore (as we use the likelihood, but do not just evaluate it at the observation); however, the likelihood principle itself does not seem very reasonable if the model is misspecified in the first place<d-cite key="jewson2018principles"></d-cite>.</p> <h2 id="generalized-variational-inference">Generalized Variational Inference</h2> <p>One more step towards generality and we find the approach presented in <d-cite key="knoblauch2019generalized"></d-cite>.</p> <p>The idea is to start from the variational formulation of Bayes’ posterior which is attributed to Donsker and Varadhan <d-cite key="donsker1975asymptotic"></d-cite>; say we have \(n\) independent observations \(\mathbf {x} = (x_1, x_2, \ldots, x_n)\); then, it can be shown that Bayes’ posterior is defined by:</p> \[\pi(\cdot|\mathbf x) = \underset{q \in \mathcal P (\Theta)}{\operatorname{argmin}}\left\{\mathbb{E}_{q(\theta)}\left[-\sum_{i=1}^{n} \log \left(p\left(x_{i} \mid {\theta}\right)\right)\right]+D_{\text{KL}}(q \| \pi)\right\},\] <p>where \(\Theta\) denotes the values that \(\theta\) can take, \(\mathcal P (\Theta)\) is the set of all probability distributions on \(\Theta\), and \(D_{\text{KL}}(q \| \pi)\) is the KL divergence between the distribution \(q\) and the prior \(\pi\).</p> <p>This formulation leads to an <em>optimization-centric</em> view of Bayesian inference, as the authors of <d-cite key="knoblauch2019generalized"></d-cite> put it; additionally, the loss-based posterior in Eq. \eqref{Eq:bissiri} <d-cite key="bissiri2016general"></d-cite> can be obtained in a similar fashion by just replacing the negative log-likelihood with the generic loss function \(\ell\):</p> \[\pi_{\ell,w}(\cdot|\mathbf x) = \underset{q \in \mathcal P (\Theta)}{\operatorname{argmin}}\left\{\mathbb{E}_{q(\theta)}\left[w \sum_{i=1}^{n} \ell(\theta, x_i) \right]+D_{\text{KL}}(q \| \pi)\right\}.\] <p>This formulation open the doors to additional extensions by changing \(D_{\text{KL}}\) in the optimization objective above to a different divergence, say \(D\). Further, you can also restrict the minimization problem to a constrained space of distributions \(\mathcal \Pi \subseteq \mathcal P(\Theta)\). The resulting update rule obtained in this way is termed <em>the Rule of Three (RoT)</em>, as you obtain a posterior distribution by specifying the loss, the space of distributions \(\mathcal \Pi\) and the divergence \(D\):</p> \[\pi_{\ell, D, \mathcal \Pi}(\cdot|\mathbf x) = \underset{q \in \mathcal \Pi }{\operatorname{argmin}}\left\{\mathbb{E}_{q(\theta)}\left[w \sum_{i=1}^{n} \ell(\theta, x_i) \right]+D(q \| \pi)\right\} \stackrel{\text{def}}{=} P(\ell, D, \Pi).\] <p>However, it is in general impossible to obtain a closed form solution for the above problem. Additionally, you do not have a coherent update anymore.</p> <p>In the paper <d-cite key="knoblauch2019generalized"></d-cite>, they show how the above problem generalizes standard Variational Inference (VI, in which you select the distribution in a given class which minimizes the KL divergence from the exact posterior), hence the name of the approach. Therefore, the RoT unifies under the same hat variational inference, loss based posteriors and the standard Bayes’ posterior. It is important to notice how, instead, VI approaches which employ different divergences from the KL (which they call divergence VI) are not included<d-footnote>This means that standard VI (with the KL) is theoretically optimal with respect to divergence VI as the former directly approximates the exact posterior. That however is restricted to the case in which the likelihood and prior are well specified, or to the case in which the variational family $\Pi$ is large enough; if that is not the case, it may be that divergence VI gives better results (as it is sometimes observed in the literature; see Section 2.4 in <d-cite key="knoblauch2019generalized"></d-cite>).</d-footnote>.</p> <p>The rest of the work discusses in very much detail (more than 100 pages!) the properties of the resulting posterior. The key idea is that with this approach you can not only perform inference with a generic loss (as it was already with the loss-based approach by <d-cite key="bissiri2016general"></d-cite> in Eq. \eqref{Eq:bissiri}), but also choose which properties of the prior to consider by picking the right \(D\), and finally reducing the computational burden posed by an unconstrained \(\mathcal P(\Theta)\) by choosing a suitable space of probability distributions \(\Pi\). That therefore tackles the other issues with standard Bayesian inference that I mentioned above.</p> <p>They also provide an axiomatic derivation of the RoT; interestingly, among the required axioms is a generalized likelihood principle, which states that all information on \(\theta\) contained in \(\mathbf x\) is accessible through evaluation of the loss \(\ell\) at the datapoints.</p> <h2 id="recap">Recap</h2> <p>I have gone up the ladder of generality, from standard Bayes’ posterior to the General Variational Inference formulation; at each step, some features of standard Bayes’ posterior are lost and some others are retained, but a larger set of tools become available. Those may work better for instance with misspecified models, or in case you do not even want to specify a full probabilistic model, or finally if a full Bayesian analysis is too costly and you want to instead resort to a variational approach.</p> <p>The following Venn’s diagram represents the relation between the different techniques presented here:</p> <div class="l-body"> <img class="img-fluid" src="/assets/img/blog/2021-08-05-generalizedBayes/venn_diagram.svg"/> </div> <h1 id="conclusion">Conclusion</h1> <p>I have followed here a possible generalization route, but that is by no means the only possibility and my overview does not include all methods which are a superset of standard Bayes’ posterior - I have for instance excluded the approach taken in <d-cite key="fong2021martingale"></d-cite> which puts the focus on the predictive distribution rather than the likelihood, or the one in <d-cite key="masegosa2019learning"></d-cite>, which uses PAC-Bayesian bounds to define distributions which do not concentrate on one single parameter value in the limit of infinite data if the model is misspecified, thus obtaining better predictive performance.</p> <p>Still, I think the approaches reviewed here are thought-provoking and allow to see Bayesian inference under a different light. I also feel they bring the original axiomatic theory closer to a pragmatical toolbox.</p> <p>Of course, I do not think now that standard Bayesian inference has to be thrown to the garbage; there is no need to list the practical results that have been possible with it. Still, it is good to know there are ways around (or beyond) in case a standard Bayesian analysis doesn’t work. Additionally, some of the techniques developed to work with standard posteriors (for instance MCMC and VI) can be ported to generalized posteriors.</p> <p>I am sure there will be other extensions proposed in the future, so keep an eye out if you are interested!</p>]]></content><author><name>Lorenzo Pacchiardi</name></author><summary type="html"><![CDATA[Updating a 250 years old theorem for the 21st century]]></summary></entry><entry><title type="html">Sampling from doubly intractable distributions - the ExchangeMCMC algorithm</title><link href="http://lorenzopacchiardi.me//blog/2020/exchangeMCMC/" rel="alternate" type="text/html" title="Sampling from doubly intractable distributions - the ExchangeMCMC algorithm"/><published>2020-12-29T00:00:00+00:00</published><updated>2020-12-29T00:00:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2020/exchangeMCMC</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2020/exchangeMCMC/"><![CDATA[<h1 id="motivation">Motivation</h1> <p>In Bayesian Statistics, the main object of interest is the posterior distribution of the parameters \(\theta\) given some observed data \(x\). Specifically, given a prior distribution \(\pi(\theta)\) and a likelihood \(p(x\vert \theta)\), the posterior is (via Bayes’ theorem):</p> \[\pi(\theta\vert x) = \frac{\pi(\theta) p(x\vert \theta)}{p(x)}, \qquad p(x) = \int \pi(\theta) p(x\vert \theta) d\theta.\] <p>Markov Chain Monte Carlo (MCMC) techniques are widely used to sample from \(\pi(\theta\vert x)\); these usually require access to \(\pi(\theta)\) and \(p (x\vert\theta)\) but do not require evaluating \(p(x)\) (we show this below for the Metropolis-Hastings algorithm).</p> <p>However, there are cases in which the likelihood function is known only up to a normalizing constant:</p> <p>\(p(x\vert \theta) = \frac {\tilde{p}(x\vert \theta)}{Z(\theta)},\) where \(\tilde p(x\vert \theta)\) is an unnormalized version of the likelihood and \(Z(\theta)\) is intractable. The posterior in this case is said <strong>doubly intractable</strong> as there are two sources of intractability: \(p(x)\) and \(Z(\theta)\).</p> <p>In this case, MCMC cannot be straightforwardly used. Below, we describe a popular extension of Metropolis-Hastings that works in the case of doubly intractable distributions; but first, let us recall the original Metropolis-Hastings algorithm.</p> <h1 id="metropolis-hastings-algorithm">Metropolis-Hastings algorithm</h1> <p>The standard Metropolis-Hastings algorithm uses a distribution \(q ( \cdot \vert \theta)\) to generate a proposal value \(\theta'\) starting from the current state of the chain \(\theta\); then, this proposal is accepted with probability \(\min\{1, \alpha \}\), where:</p> <p>\begin{equation} \alpha = \frac{q(\theta\vert \theta’) \pi(\theta’) p(x\vert \theta’)}{q(\theta’\vert \theta) \pi(\theta) p(x\vert \theta)} = \frac{q(\theta\vert \theta’) \pi(\theta’) \tilde {p}(x\vert \theta’) }{q(\theta’\vert \theta) \pi(\theta) \tilde {p}(x\vert \theta)} \textcolor{red}{\frac{Z(\theta)}{Z(\theta’)}}. \end{equation}</p> <p>As you can see, this does not require knowing \(p(x)\), but instead requires the ratio \({Z(\theta)}/{Z(\theta')}.\) It cannot therefore be applied in the setting of doubly-intractable distributions.</p> <div class="l-body"> <img class="img-fluid" src="/assets/img/blog/2020-12-30-exchangeMCMC/alg1.svg"/> </div> <h1 id="the-exchangemcmc-algorithm">The ExchangeMCMC algorithm</h1> <p>The ExchangeMCMC algorithm (Murray et al., 2012, <d-cite key="murray2012mcmc"></d-cite>) bypasses this issue by drawing an auxiliary observation</p> \[x' \sim p(\cdot\vert \theta')\] <p>and defining the acceptance probability as:</p> <p>\begin{equation}\label{Eq:acc_rate_exchange} \alpha = \frac{q(\theta\vert \theta’) \pi(\theta’)p(x\vert \theta’)}{q(\theta’\vert \theta)\pi(\theta) p(x\vert \theta)}\textcolor{blue}{\frac{p(x’\vert \theta)}{p(x’\vert \theta’)}} = \frac{q(\theta\vert \theta’) \pi(\theta’)\tilde p(x\vert \theta’)\textcolor{blue}{\tilde p(x’\vert \theta)}}{q(\theta’\vert \theta)\pi(\theta)\tilde p(x\vert \theta)\textcolor{blue}{\tilde p(x’\vert \theta’)}} \frac{\cancel{Z(\theta)\textcolor{blue}{Z(\theta’)}}}{\cancel{Z(\theta’)\textcolor{blue}{Z(\theta)}}}, \end{equation} where the black part is the same as in the original Metropolis-Hastings algorithm, but adding the blue terms allow to evaluate \(\alpha\) without estimating the normalizing constants. Still, the resulting algorithm targets the correct distribution.</p> <div class="l-body"> <img class="img-fluid" src="/assets/img/blog/2020-12-30-exchangeMCMC/alg2.svg"/> </div> <h1 id="some-extensions">Some extensions</h1> <h2 id="bridging">Bridging</h2> <p>The acceptance rate in Eq.\eqref{Eq:acc_rate_exchange} depends on two ratios: \(\frac{p(x\vert \theta')}{p(x\vert \theta)}\) represents how well the proposed parameter value explains the observation with respect to the previous parameter value, while \(\frac{p(x'\vert \theta)}{p(x'\vert \theta')}\) measures how well the auxiliary variable (generated using \(\theta'\)) can be explained with parameter \(\theta\).</p> <p>Therefore, even if the former is large and \(\theta'\) would be a suitable parameter value, \(\alpha\) can still be small if the auxiliary random variable is not explained well by the previous parameter value. This can lead to slow mixing of the chain; to improve on this, Murray et al., 2012, proposed to sample a set of auxiliary variables \((x'_0, x'_1, \ldots, x'_K)\) from intermediate distributions in the following way: consider a set of densities: \(\tilde p_k(x \vert \theta, \theta') = \tilde p(x\vert \theta')^{\beta_k} \tilde p(x\vert \theta)^{1- \beta_k}, \quad \beta_k = \frac{K - k + 1}{K + 1};\) \(x'_0\) is generated from \(p(\cdot\vert \theta')\) as before, and then each \(x'_k\) is generated from \(R(\cdot \vert x'_{k-1}; \theta, \theta')\), which denotes a Metropolis-Hastings transition kernel starting from \(x'_{k-1}\) with stationary density \(\tilde p_k(\cdot\vert \theta, \theta')\). Then, the acceptance rate is modified as follows: \begin{equation}\label{Eq:acc_rate_exchange_bridging} \alpha = \frac{\tilde p(x\vert \theta’)q(\theta\vert \theta’) \pi(\theta’)}{\tilde p(x\vert \theta)q(\theta’\vert \theta)\pi(\theta)} \cdot \prod_{k=0}^{K} \frac{\tilde p_{k+1}(x’_k\vert \theta, \theta’) }{\tilde p_k(x’_k\vert \theta, \theta’) }. \end{equation}</p> <p>Note that \(K=0\) recovers the original ExchangeMCMC. This procedure, usually called <em>bridging</em>, generally improves the acceptance rate as it basically introduces a sequence of intermediate updates to the auxiliary data which by smoothening out the difference between the two distributions.</p> <div class="l-body"> <img class="img-fluid" src="/assets/img/blog/2020-12-30-exchangeMCMC/alg3.svg"/> </div> <h2 id="exchangemcmc-without-perfect-simulations">ExchangeMCMC without perfect simulations</h2> <p>If it is not possible to sample from \(p(\cdot\vert \theta')\) as it is required in the ExchangeMCMC algorithm, Murray et al., 2012, suggested to run \(T_{in}\) steps of an MCMC chain on \(x\) targeting \(p(\cdot\vert \theta')\) at each step of ExchangeMCMC; if \(T_{in}\) is large enough, the last sample can be considered as (approximately) drawn from \(p(\cdot\vert \theta')\) itself and used in place of the unavailable perfect simulation.</p> <p>In practice, however, this only leads to an approximate ExchangeMCMC algorithm, as at each iteration of the inner chain a finite \(T_{in}\) is used, so that the inner chain would not perfectly converge to its target; for this reason, even an infinitely long outer chain would not target the right posterior for any finite \(T_{in}\).</p> <p>Nonetheless, this approach was shown empirically to work satisfactorily <d-cite key="liang2010double"></d-cite> <d-cite key="caimo2011bayesian"></d-cite> <d-cite key="everitt2012bayesian"></d-cite>; moreover, it has been argued <d-cite key="liang2010double"></d-cite> that starting the inner chain from the observation value improves convergence.</p> <p>Some theoretical guarantees (albeit under strongs conditions), are given in Appendix B of Everitt, 2012 <d-cite key="everitt2012bayesian"></d-cite>, which bounds the total variation distance between the target of approximate ExchangeMCMC with finite \(T_{in}\) and the target of the exact one, and shows that they become equal when \(T_{in} \to \infty\).</p> <div class="l-body"> <img class="img-fluid" src="/assets/img/blog/2020-12-30-exchangeMCMC/alg4.svg"/> </div> <h1 id="related-algorithms">Related algorithms</h1> <p>ExchangeMCMC algorithm is very popular due to its ease of use. However, it is certainly not the only algorithm to sample from doubly intractable distribution (see the nice review given in Park and Haran, 2018 <d-cite key="park2018bayesian"></d-cite>); some are listed next:</p> <ul> <li>Caimo et al., 2011 <d-cite key="caimo2011bayesian"></d-cite> proposed a parallel-chain MCMC algorithm</li> <li>Everitt et al., 2012<d-cite key="everitt2017marginal"></d-cite> built an SMC-type algorithm which is also capable of recycling information from past simulations</li> <li>Finally, Liang et al., 2016 <d-cite key="liang2016adaptive"></d-cite> proposed an algorithm which is inspired from ExchangeMCMC and, in the case of impossible perfect sampling, still targets the right invariant distribution. This algorithm requires some hand-tuning and needs to keep in memory a large amount of data, which may hinder its applicability.</li> </ul> <p>Differently from ExchangeMCMC, the above ones can be parallelized.</p>]]></content><author><name>Lorenzo Pacchiardi</name></author><summary type="html"><![CDATA[A quick review]]></summary></entry></feed>