<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://lorenzopacchiardi.me//feed.xml" rel="self" type="application/atom+xml"/><link href="http://lorenzopacchiardi.me//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-11T14:50:17+00:00</updated><id>http://lorenzopacchiardi.me//feed.xml</id><title type="html">blank</title><subtitle>Lorenzo Pacchiardi, Research Associate at the Center for the Future of Intelligence, University of Cambridge. </subtitle><entry><title type="html">Summary of “Forecasting Rare Language Model Behaviors”</title><link href="http://lorenzopacchiardi.me//blog/2025/rare/" rel="alternate" type="text/html" title="Summary of “Forecasting Rare Language Model Behaviors”"/><published>2025-03-24T15:09:00+00:00</published><updated>2025-03-24T15:09:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2025/rare</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2025/rare/"><![CDATA[<p>A new <a href="https://arxiv.org/abs/2502.16797">paper</a> by Anthropic shows how to predict the max probability that a query out of a set of n queries elicits a particular (unwanted) model behaviour.</p> <p>Motivation: in deployment, models are tested on a much larger number of queries than in evaluation. Each specific query \(x\) (prompt) causes the model to produce an (unwanted) outcome with a given probability (by repeated sampling):</p> \[p_{\text {ELICIT }}\left(x ; \mathcal{D}_{LLM}, B\right)=\mathbb{E}_{o \sim \mathcal{D}_{\mathrm{LIM}}(x)} \mathbf{1}[B(o)=1]\] <p>where \(\mathcal D_{LLM}(x)\) indicates the sampling distribution of the model given \(x\) , \(o\) indicates a given output and \(B\) is an indicator function for a particular behaviour. The probability \(p_{\text {ELICIT }}\) may be small but not negligible, and it may be estimated by repeated sampling using query \(x\) . The paper then asks: by considering a distribution of queries \(\mathcal D_x\) (assumed to be the same in evaluation and deployment), how does the largest observed value of \(p_{\text {ELICIT }}\) scale with the number of queries \(n\) sampled from \(\mathcal D_x\) ? In fact, the paper motivates (sec 3.2) that the largest value of \(p_{\text {ELICIT }}\) is a good representation of the aggregate risk over the considered \(n\) queries and the frequency of times the behaviour is expected to appear. Notice that \(\mathcal D_x\) induces a distribution on \([0,1]\) through the computation of \(p_{\text {ELICIT }}\) ; therefore, the paper asks how the top \(1/n\) quantile of the distribution of \(p_{\text {ELICIT }}\) , indicated as \(Q_p(n)\) , scales with \(n\) :</p> \[\mathbf{P}_{x \sim \mathcal{D}x}\left[p_{\text {ELICIT }}\left(x ; \mathcal{D}_{\mathrm{LLM}}, B\right) \geq Q_p(n)\right]=1 / n\] <p>The paper then recurs to techniques from extreme value theory and finds that the \(Q_p(n)\) and \(n\) are linearly linked in log-space for large \(n\)</p> \[\log \left(-\log Q_p(n)\right)=\frac{1}{a} \log n-\frac{b}{a},\] <p>thus allowing to predict \(Q_p(n)\) for higher orders of magnitude from experiments conducted on smaller ones. They test this in practice and achieve good results.</p> <p>The method is not perfect, as the authors acknowledge; for instance, it assumes the evaluation and deployment distribution of prompts are identical, therefore without accounting for distribution shift or adversarial adaptation. However, it is a great first step.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Using extreme value theory to predict the max elicitation probability in a set of prompts]]></summary></entry><entry><title type="html">Predicting when LLMs fail</title><link href="http://lorenzopacchiardi.me//blog/2025/UQ-assessor/" rel="alternate" type="text/html" title="Predicting when LLMs fail"/><published>2025-01-03T00:00:00+00:00</published><updated>2025-01-03T00:00:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2025/UQ-assessor</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2025/UQ-assessor/"><![CDATA[<p><em>Updated on 30th Jan 2025 by adding three papers</em></p> <h1 id="intro">Intro</h1> <p>Large Language Models (LLMs) are ML systems. As for any ML system, we may want to predict whether they will fail or succeed on specific task instances (e.g., a specific question from a QA dataset).</p> <p>There are several interrelated paradigms to do so, differing in whether they are specific to a model or not and whether they rely on the model’s outputs or internals. The aim of this post is to frame how these different approaches relate to one another. Therefore, I overview below three main paradigms, and point towards closely related research fields that tackle different questions.</p> <p><em>Disclaimer:</em> <em>I do not claim this overview to cover all existing research strands related to predicting LLM failures or to comprehensively capture all works in each of the considered ones. I welcome any feedback.</em></p> <p>TL;DR: in brief, I identified three paradigms that can predict where LLMs fail. The third one is likely very hard to apply to LLMs in practice. The table below summarises their properties:</p> <ul> <li>whether additional training is required</li> <li>whether the learned approach is model-specific</li> <li>whether the input must be passed through the model to predict whether the latter will fail or not</li> </ul> <table border="1"> <thead> <tr> <th>CATEGORY</th> <th>TRAINING</th> <th>MODEL-SPECIFIC</th> <th>PASSING INPUT THROUGH THE MODEL</th> </tr> </thead> <tbody> <tr> <td>Intrinsic UQ</td> <td>In some cases (eg whitebox methods)</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>Assessor</td> <td>Yes</td> <td>Yes (they can be trained to work for a set of models, based on model features alongside instance features)</td> <td>No</td> </tr> <tr> <td>Model-agnostic rejector</td> <td>Yes, but once per data distribution</td> <td>No</td> <td>No</td> </tr> </tbody> </table> <h1 id="paradigms-to-predict-where-llms-fail">Paradigms to predict where LLMs fail</h1> <h2 id="intrinsic-uncertainty-quantification">Intrinsic uncertainty quantification</h2> <p>Uncertainty quantification (UQ) has been extensively researched in “traditional” deep learning. It involves a suite of techniques aimed at extracting an uncertainty quantification from a trained model (such as reporting the logits of a trained multi-class classifier network) or training those models to embed a form of uncertainty (such as Bayesian Neural Networks, or BNNs). Most methods falling under the term “uncertainty quantification” in the traditional deep learning literature involve producing an output with the model and accompanying it with an uncertainty estimate obtained in some way.</p> <p>Some of these methods have been adapted or evolved into approaches applicable to LLMs; however, other traditional methods are hardly applicable to them. In fact, with respect to traditional deep learning, LLMs have two main differences that limit the applicability of existing methods and spurred the creation of novel ones:</p> <ul> <li>first, training a LLM is extremely costly. As such, it is hard to apply uncertainty quantification methods that require bespoke training approaches (such as BNNs).</li> <li>second, LLMs do not perform regression or classification, but rather language generation in an auto-regressive manner: to answer a specific question, a LLM recursively generates new tokens to produce a suitable completion. Even if token-level logits (and thus probabilities) are immediately available, it is not trivial to obtain an answer-level uncertainty estimate from them. This therefore spurred the creation of novel approaches specific to LLMs.</li> </ul> <p>Shorinwa et al., 2024, <d-cite key="shorinwa2024survey"></d-cite> provides a nice overview of existing UQ methods for LLMs and their connection with classical ones. Beyond token-level uncertainty quantification, a second class of methods involve having the models verbalise their uncertainty, either after or before an answer has been produced <d-cite key="kadavath2022language"></d-cite><d-cite key="lin2022teaching"></d-cite><d-cite key="kapoor2024large"></d-cite>. A third category is that of “semantic similarity” approaches; for instance, Kuhn et al., 2023, <d-cite key="kuhn2023semantic"></d-cite> generates different answers, clusters them according to meaning and considers the uncertainty over clusters. Finally, a set of “white-box” works rely on training additional modules that, starting from model activations at a given internal layer, predicts the probability of a produced answer being correct <d-cite key="kadavath2022language"></d-cite><d-cite key="ferrando2024entity"></d-cite>. To extend the applicability of those methods to black-box LLMs, a recent work <d-cite key="sam2025predicting"></d-cite> ask a set of “elicitation questions” after an answer has been generated and uses the binary responses (or log probs, if available) to predict whether the answer was correct, thus replacing the representation obtained from model internals with the responses to those questions.</p> <p>In general, there does not seem to be complete consensus on whether uncertainty quantification methods for LLMs work well. See Sections 1 and 2 of <d-cite key="kapoor2024large"></d-cite> for a nice overview. For instance, a recent work <d-cite key="pawitan2024confidence"></d-cite> has shown how the model confidence obtained by those different methods is not necessarily consistent: indeed, they find that the token-level probabilities and verbalised confidence are very poorly correlated.</p> <p>The approaches above differ in terms of level access: from access to activations (white-box), to log-probabilities (token-level approaches), to necessity of multiple generation (as for some semantic similarity approaches), to simple generation for verbalised UQ. Across all of those categories, some works involve finetuning the model to improve its UQ ability <d-cite key="kadavath2022language"></d-cite>.</p> <p>Nevertheless, all UQ approaches for LLMs require to produce model outputs (or even multiple versions), or at least to pass the input through the LLM to obtain its activations.</p> <h2 id="assessors-anticipating-llm-performance">Assessors: anticipating LLM performance</h2> <p>An alternative paradigm which has received less attention is that of “assessors” <d-cite key="hernandez2022training"></d-cite>: additional modules that are trained to predict the correctness of a main model on specific instances <strong>without</strong> relying on that model’s output or activations <d-footnote>The original nomenclature also included assessors relying on LLM output, which would make the “white-box” unceratinty quantification methods also fall under this. For ease of understanding, I will then consider all assessors to not rely on LLM outputs</d-footnote>. In practice, assessors are classifiers that tackle the problem of predicting the success of the main model starting from a set of features of the input <d-cite key="schellaert2024analysing"></d-cite><d-cite key="zhou2022reject"></d-cite>, such as some embeddings. This makes the assessor model-specific. Alternatively, a single assessor can be trained to predict the success of multiple models, by relying on specific features to identify them <d-cite key="pacchiardi2024instances"></d-cite>, thus predicting success of a model on a specific instance from a pair (instance features, model features).</p> <p>In both cases, the main point is that the model does not require to see the input at test time for the assessor to make a prediction of its performance. This has two advantages:</p> <ul> <li>first, it reduces the computational cost of running an input through an LLM if it will predictably fail.</li> <li>second, it makes the assessor robust to manipulation: if an LLM receives feedback from an assessor that uses its outputs, the LLM could learn to produce manipulative output, similarly to what happens with RLHF <d-cite key="wen2024mislead"></d-cite>.</li> </ul> <p>The concept of assessors is closely related to that of routers <d-cite key="hu2024routerbench"></d-cite>, which are modules that predict which of a set of LLMs is more likely to succeed at a specific task instance.</p> <h2 id="model-agnostic-rejectors">Model-agnostic rejectors</h2> <p>Model-agnostic rejectors operate on the assumption that a model is likely to fail when confronted with a data point significantly different from the training distribution. This concept is closely aligned with anomaly detection. To implement this, a predictor is trained on the training data to identify inputs that diverge notably from that distribution.</p> <p>I am unaware of any work applying this idea to LLMs; I believe the main reason is that LLMs’ training datasets are vast and often unknown, so it is basically impossible to train a rejector using information on that. Even more importantly, as LLMs can tackle a wide range of tasks, it would not make sense to train a rejector on the pre-training dataset.</p> <p>Therefore, the practical usefulness of this approach may be limited to cases where LLMs undergo fine-tuning for specific use cases in niche domains. Even here, however, it may be better to rely on assessors or uncertainty quantification methods which, by exploiting information on the model, are likely to perform better. In fact, the main advantage of model-agnostic rejectors is that they can be applied to any model trained on a specific dataset; however, it is rare to have multiple LLMs trained (or even finetuned) on the same dataset, due to their large cost.</p> <h2 id="bonus-modifying-the-llm-itself">Bonus: Modifying the LLM Itself</h2> <p>A recent study <d-cite key="cohen2024idk"></d-cite> introduced an innovative approach by integrating an “I don’t know” token into the LLM’s vocabulary during the pre-training phase. This addition improved the model’s calibration by enabling it to explicitly express uncertainty.</p> <p>Such advancements echo techniques in uncertainty quantification (UQ) for traditional deep learning, where models are modified or trained in bespoke ways to better capture uncertainty.</p> <h1 id="tangential-directions">Tangential directions</h1> <h2 id="extracting-other-information-from-model-internals">Extracting other information from model internals</h2> <p>There are other works investigating the use of model internals to extract additional insights. For instance, some studies <d-cite key="burger2024truth"></d-cite><d-cite key="azaria2023internal"></d-cite> focus on determining whether a model is engaging in deceptive behavior (commonly referred to as “lying”).</p> <p>This approach bears similarities to extracting uncertainty quantification for LLMs but instead targets different “functions” of a response beyond mere correctness. It could also prove valuable in identifying instances of “sandbagging” <d-cite key="weij2024sandbagging"></d-cite>, where models strategically underperform under specific conditions.</p> <h2 id="using-llms-for-forecasting">Using LLMs for forecasting</h2> <p>Rather than focusing on quantifying uncertainty in LLM-generated answers, this approach leverages LLMs to provide uncertainty quantification for external world events. This is conceptually akin to applying intrinsic model uncertainty to simple QA quizzes but extended to inherently uncertain questions. Instead of addressing questions with definitive answers, this method deals with questions characterized by intrinsic unpredictability. Notable efforts in this domain include:</p> <ul> <li>Dataset: The Autocast dataset <d-cite key="zou2022forecasting"></d-cite> compiles a collection of questions and aggregated human forecasts derived from forecasting tournaments, complemented by a news corpus. In these tournaments, forecasters tackle True/False, multiple-choice, and numerical questions. After the resolution of each question, forecasters are scored using Scoring Rules based on the forecasts they provided at earlier dates. An LLM can simulate forecasting for a specific date by processing news articles up to that date and attempting to answer the questions. The diverse question formats and the extensive numerical ranges make this a challenging benchmark. Consequently, Autocast serves as an outstanding real-world testbed for developing scalable uncertainty quantification methods tailored to LLMs.</li> <li>Experiments: Recent studies <d-cite key="schoenegger2024wisdom"></d-cite><d-cite key="halawi2024forecasting"></d-cite> have explored the use of agentic architectures, enabling LLMs to actively engage in forecasting tasks. These experiments push the boundaries of LLM capabilities in addressing uncertainty in dynamic, real-world scenarios.</li> </ul> <h2 id="reward-models">Reward models</h2> <p>Reward models can be considered as somehow related to uncertainty quantification: they are typically trained (for instance from human feedback) to evaluate a completion in terms of a validity specification. However, this validity specification usually includes absence of toxic content or alignment with user intentions, rather than correctness of an answer, even if the latter could plausibly be used. Then, they are used as the reward function to finetune LLMs via RL. However, they can also be independently used to evaluate completions produced by a model; some works also show that they can be used to evaluate partial completions, thus stopping generation and then restarting it if a generation is likely to be unsuccessful wrt the final specification <d-cite key="nath2024goal"></d-cite>.</p> <p>However, in contrast to intrinsic uncertainty quantification and assessors, reward models are generally not specific to a single LLM. As such, they rely on generic features of the output that can be used to understand agreement with the considered validity specification. In contrast, assessors and uncertainty quantification approaches are trained considering a specific model or a set of models and explicitly or implicitly rely on their features.</p> <h2 id="how-humans-interpret-llm-uncertainty">How humans interpret LLM uncertainty</h2> <p>Human perception and prediction of LLM performance represents another important research direction. Studies have revealed significant limitations in human ability to anticipate LLM behavior. A recent experiment <d-cite key="carlini2024gpt"></d-cite> has shown that humans perform only marginally better than random chance when predicting GPT-4’s performance. Moreover, humans tend to exhibit overconfidence in predicting LLM performance on high-stakes tasks, likely due to anchoring on past interactions with these systems <d-cite key="vafa2024large"></d-cite>. A particularly concerning finding is that LLM success on challenging tasks within a domain does not necessarily translate to reliable performance on simpler tasks in the same domain <d-cite key="zhou2024larger"></d-cite>. This counterintuitive behavior challenges our natural assumptions about machine learning systems and highlights the importance of systematic evaluation approaches. Additionally, a recent work <d-cite key="steyvers2025large"></d-cite> shows that, even for LLMs that are well-calibrated in terms of the probabilities they assign to the tokens indicating the options in multiple-choice scenarios, the explanations they provide to justify the chosen option lead humans to become overconfident in whether the LLM is correct (the authors term this the “calibration gap”). Moreover, the authors show that longer LLM explanations increase the confidence of users even if accuracy does not improve. However, they find that it is possible to adjust LLM explanations to better reflect the models internal confidence, which allows to reduce the calibration gap. Another work <d-cite key="kapoor2024large"></d-cite> subject humans to a test with the assistance of an LLM with several uncertainty quantification methods (more or less calibrated) and find that users modulate their choices following the confidence outputs, indicating that improving calibration of those confidences is important.º</p> <h2 id="robustness-of-llms-to-input-perturbation">Robustness of LLMs to input perturbation</h2> <p>The sensitivity of LLMs to minor prompt modifications has been well-documented in the literature <d-cite key="dhole2022augmenter"></d-cite><d-cite key="shen2024jailbreak"></d-cite>. Research has demonstrated that even subtle changes to input prompts can result in substantially different outputs, raising questions about the reliability and consistency of these systems. This sensitivity to input perturbations suggests a potential avenue for uncertainty estimation: by measuring a model’s response variability to small input modifications, we could potentially identify cases where the model exhibits high uncertainty and should be rejected. While this approach has been theoretically proposed in recent literature reviews, particularly in Hendrickx et al.’s 2024 review <d-cite key="hendrickx2024reject"></d-cite>, its practical application to LLMs remains unexplored.</p> <h1 id="further-reading">Further reading</h1> <ul> <li>A comprehensive introduction to uncertainty quantification in NLP can be found in a <a href="https://sites.google.com/view/uncertainty-nlp">2022 tutorial</a> that covers fundamental concepts and key techniques in the field. While slightly dated, it provides valuable foundational knowledge.</li> <li>Hendrickx et al.’s 2024 review <d-cite key="hendrickx2024reject"></d-cite> on machine learning with reject option offers broader insights beyond the LLM context. The review introduces a useful taxonomy, distinguishing between novelty rejection (for inputs substantially different from training data) and ambiguity rejection (for cases where training data or learned algorithms show uncertainty). It proposes three architectural principles: <ul> <li>Separate rejectors operate independently of any predictor, aligning with the model-agnostic rejector paradigm discussed earlier. Notably, while assessors don’t rely on model outputs, they differ from separate rejectors as they incorporate information about predictor performance.</li> <li>Dependent rejectors utilize predictor outputs, often through confidence functions or input perturbation techniques, encompassing many of the UQ approaches outlined above.</li> <li>Integrated rejectors combine rejection capability within the primary model, typically by introducing an additional “reject” class. This category includes both certain UQ methods and approaches involving LLM modification during training.</li> </ul> </li> </ul>]]></content><author><name>Lorenzo Pacchiardi</name></author><summary type="html"><![CDATA[An overview of different paradigms to do so and connections with related areas]]></summary></entry><entry><title type="html">Summary of “From Testing to Evaluation of NLP and LLM Systems”</title><link href="http://lorenzopacchiardi.me//blog/2024/substack1/" rel="alternate" type="text/html" title="Summary of “From Testing to Evaluation of NLP and LLM Systems”"/><published>2024-12-31T17:39:00+00:00</published><updated>2024-12-31T17:39:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2024/substack1</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2024/substack1/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This work compares academic research in evaluation with practitioners' questions on community forums.]]></summary></entry><entry><title type="html">Finetuning GPT3</title><link href="http://lorenzopacchiardi.me//blog/2022/GPT3_finetuning/" rel="alternate" type="text/html" title="Finetuning GPT3"/><published>2022-11-26T15:09:00+00:00</published><updated>2022-11-26T15:09:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2022/GPT3_finetuning</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2022/GPT3_finetuning/"><![CDATA[<p>Large Language Models have quite extraordinary performance. They can generate text, translate languages, and even answer questions. However, they are not perfect (fun fact: this last sentence was actually suggested by Github copilot, which is powered by a language model :laughing:). If you have a specific task, you may want to finetune the model on your own dataset.</p> <p>OpenAI API allows you to do that. You can finetune GPT3 and then using it as you wish. In the notebook below, I show how to finetune GPT3 to predict the title of an arxiv paper from its abstract. Hope you enjoy it!</p> <p><a href="https://colab.research.google.com/gist/LoryPack/c53b41e7041f77ac450f7394455d2d2e#scrollTo=GwQeLZWFGs7_"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p> <p>One disclaimer: to use the OpenAI API you need to register and using it has a monetary cost.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Using the OpenAI API to finetune GPT3 on a custom dataset]]></summary></entry><entry><title type="html">Generalizing Bayesian Inference</title><link href="http://lorenzopacchiardi.me//blog/2021/generalizedBayes/" rel="alternate" type="text/html" title="Generalizing Bayesian Inference"/><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2021/generalizedBayes</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2021/generalizedBayes/"><![CDATA[<p>If you are reading this, you probably know what Bayes’ theorem is. Here, we are concerned with the use of Bayes’ theorem to perform inference for parameters of a statistical model (a.k.a. <strong>Bayesian inference</strong>).</p> <p>Recently, several generalizations of Bayesian inference have been proposed. The aim of this post is to survey some of these generalizations and overview the pros and cons of each with respect to using the original Bayes theorem. Hopefully, I’ll be able to put some order in this rapidly expanding literature and provide some intuition on why we need to move beyond the original Bayes theorem.</p> <p><strong>TL; DR:</strong> Extensions of Bayesian inference work better in some cases which are not contemplated by using the original Bayes’ theorem (primarily, model misspecification), and provide more justification.</p> <h1 id="the-original-bayes-theorem">The original Bayes theorem</h1> <div style="text-align:center;font-size:15px;"> <div class="l-body"> <img class="img-fluid" src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif"/> </div> <i> A portrait of Thomas Bayes (maybe). Source: <a href="https://commons.wikimedia.org/wiki/File:Thomas_Bayes.gif">Unknown author</a>, Public domain, via Wikimedia Commons.</i> </div> <p><br/> Let’s first have a look at Bayes’ original theorem <d-footnote>This is actually the form that was given to it by Laplace; click <a href="https://www.lesswrong.com/posts/RTt59BtFLqQbsSiqd/a-history-of-bayes-theorem" target="_blank"> here </a> for a nice history of the theorem. </d-footnote>:</p> \[\pi(\theta\vert x) = \frac{\pi(\theta) p(x\vert \theta)}{p(x)}, \qquad p(x) = \int \pi(\theta) p(x\vert \theta) d\theta.\] <p>Here, I am using \(\theta\) to denote a <em>parameter</em> on which we want to perform inference, while \(x\) is the <em>data</em> that we observe.</p> <p>As the standard textbook description of Bayesian inference says, Bayes’ theorem provides a <em>posterior belief</em> \(\pi(\theta\vert x)\), by updating the <em>prior belief</em> \(\pi(\theta)\) with the information on \(\theta\) carried by \(x\). Crucially, all information on \(\theta\) contained in \(x\) is represented by the <em>likelihood</em> \(p(x\vert \theta)\), such likelihood being the probabilistic model from which <strong>we believe</strong> \(x\) was generated. This is the gist of Bayesian inference.</p> <p>Actually, the denominator in the definition of the posterior is independent on \(\theta\). Exploiting this fact, it is common to write:</p> \[\pi(\theta\vert x) \propto \pi(\theta) p(x\vert \theta),\] <p>where the proportional sign refers to both sides of the equation being considered as function of \(\theta\)<d-footnote>When some posterior expectations need to be computed, the function on the right-hand side needs to be normalized (ie divided by the normalizing constant $p(x)$). However, computing the normalizing constant is not needed when sampling from the posterior with MCMC methods, for instance.</d-footnote>.</p> <p>Additionally, Bayes’ theorem is very modular, allowing to sequentially incorporate new information into our belief: say that we previously had data \(x_1\), from which we obtained \(\pi(\theta\vert x_1)\). If we now get an independent observation \(x_2\), we can apply Bayes’ theorem again by using \(\pi(\theta\vert x_1)\) as a prior and get:</p> \[\pi(\theta\vert x_1, x_2) \propto \pi(\theta\vert x_1) p(x_2\vert \theta).\] <p>So, if we get \(n\) independent observations, say \(\mathbf {x} = (x_1, x_2, \ldots, x_n)\), the posterior belief is:</p> \[\pi(\theta\vert \mathbf x) = \frac{\pi(\theta) \prod_{i=1}^n p(x_i\vert \theta)}{p(\mathbf x)} \propto \pi(\theta) \prod_{i=1}^n p(x_i\vert \theta).\] <h2 id="properties-of-bayes-posterior">Properties of Bayes posterior</h2> <p>The posterior obtained from Bayes’ theorem satisfies some nice properties; first, there are properties that are intrinsic to Bayes’ update rule:</p> <ul> <li><strong>coherence:</strong> in whatever order you process the observations \(x_i\), you end up with the same posterior belief<d-footnote>This property may be given different names in other works, as for instance <i>Bayesian additivity</i>.</d-footnote>.</li> <li><strong>Likelihood principle:</strong> As the observations \(\mathbf x\) only appear through the likelihood in the definition of the posterior, Bayes’ theorem satisfies the likelihood principle, which says that (from <a href="https://en.wikipedia.org/wiki/Likelihood_principle">Wikipedia</a>): <blockquote> <p>given a statistical model, all the evidence in a sample relevant to model parameters is contained in the likelihood function.</p> </blockquote> </li> </ul> <p>If you further assumes that the observations \(\mathbf x\) are generated from \(p(\cdot\vert\theta_0)\) for some parameter value \(\theta_0\) (i.e., the model is <em>well specified</em>), Bayes’ posterior satisfies some additional properties:</p> <ul> <li> <p>according to information theory arguments in Zellner <d-cite key="zellner1988optimal"></d-cite>, Bayes theorem is the <strong>optimal way to process information</strong>, in the sense that its use does not discard any information present in the data about the parameters.</p> </li> <li> <p>An analogue to the Central Limit Theorem in frequentist statistics applies to Bayes’ posterior, called <strong>Bernstein-von Mises theorem</strong>. It goes like this: as the number of observations \(n\) goes to infinity, the posterior converges to a normal distribution which is centered in \(\theta_0\) (and whose variance decreases as \(1/n\) and is asymptotically equivalent to the sampling variance of the maximum likelihood estimator of \(\theta\))<d-footnote>The theorem of course holds under some regularity conditions, such as $\pi(\theta_0)&gt;0$; click <a href="https://encyclopediaofmath.org/wiki/Bernstein-von_Mises_theorem#References" target="_blank"> here </a> for an introduction and some further references. </d-footnote>. That means that Bayesian inference is, asymptotically, equivalent to maximum likelihood estimation, as it recovers the exact parameter value \(\theta_0\).</p> </li> </ul> <h2 id="so-what-is-wrong">So what is wrong?</h2> <p>To recap what we said above, the main motivation behind Bayes’ theorem is that, if the model \(p(x\vert \theta)\) is well specified, the posterior distribution is a coherent way to learn about the true parameter value (to which it converges in the limit of infinite data); additionally, that is the best possible way to process information even with a finite amount of observations (by Zellner’s argument).</p> <p>However, these arguments vacillate when the model is not an accurate representation of the distribution \(g(\cdot)\) from which observations were generated (or data-generating process, DGP). By this, we mean that there do not exist any parameter value such that \(p(\cdot\vert\theta)=g(\cdot)\). If that is the case, two things happen:</p> <ul> <li>Zellner’s argument simply does not hold anymore <d-cite key="holmes2017assigning"></d-cite>.</li> <li>A Bernstein von-Mises result still holds, but now the asymptotic normal distribution will be centered in the parameter value \(\theta^\star = \arg \min_{\theta} D_{\text{KL}}(g(\cdot)\vert \vert p(\cdot|\theta)),\) where \(D_{\text{KL}}\) is the Kullback-Leibler (KL) divergence<d-footnote> Notice that $\theta^\star$ is also the parameter value to which the frequentist maximum likelihood estimate converges</d-footnote>.</li> </ul> <p>More in general, we need to ask what is the aim of Bayesian inference in such a <strong>misspecified</strong> setting. In fact, with Bayesian inference we do not learn about the <em>true</em> parameter value anymore, as such thing does not exist. Rather, the standard Bayes’ posterior learns about the parameter value such that the misspecified model is as close as possible to the DGP in the specific sense of the KL divergence.</p> <p>That may still be what you want to do in some cases, but I argue below that using the KL divergence may behave poorly in some misspecified cases; instead, some generalized Bayesian inference allow the user to choose the way in which you approximate the DGP with the probabilistic model (for instance replacing the KL with other divergences <d-cite key="pacchiardi2021generalized"></d-cite><d-cite key="jewson2018principles"></d-cite>). Others instead completely dispose of a probabilistic model.</p> <h3 id="issues-with-the-kl-divergence">Issues with the KL divergence</h3> <p>Learning \(\theta^\star\) according to the KL divergence may not behave well when the model is misspecified; for instance, consider the following DGP:</p> \[g(x) = 0.1\ h(x) + 0.9\ p(x|\theta_0);\] <p>this means that 90% of observations are generated from the model for a given parameter value \(\theta_0\), while the other 10% are generated from the distribution \(h\). If \(h\) has heavier tails than \(p\), \(\theta^\star\) can be far away from \(\theta_0\), as the KL divergence gives large importance to the tail behavior of the distributions<d-cite key="jewson2018principles"></d-cite>. With a finite amount of samples, that translates into saying that Bayes’ posterior is highly sensitive to outliers in the data.</p> <p>With more general misspecifications, things may go wrong in different ways.</p> <h3 id="prior-and-computational-limits">Prior and computational limits</h3> <p>Finally, Bayesian inference implicitly assumes that the prior is a good representation of previous knowledge, and that enough computational power is available to sample from the posterior (which, in some cases, it is very expensive to do) <d-cite key="knoblauch2019generalized"></d-cite>.</p> <p>Some of the generalized Bayesian approaches <d-cite key="knoblauch2019generalized"></d-cite> explicitly include the fact that these assumptions may be broken in the definition of the inference strategy, as we will see below.</p> <h1 id="generalizations-of-bayes-posterior">Generalizations of Bayes posterior</h1> <p>In the following, I review extensions to Bayes theorem which are more justified and may have better performances in a misspecified setting (or can even be used without a probabilistic model); some will also tackle directly the issues regarding prior and computational power.</p> <p>Across these works, a recurrent underlying question is: what is the actual aim of inference when we cannot specify the model correctly?</p> <p><strong>Disclaimer</strong>: this overview is non-exhaustive and strongly biased due to papers I’ve read and my personal research activity.</p> <h2 id="power-likelihood">Power likelihood</h2> <p>A first idea to tackle (mild) model misspecification is to reduce the importance of the likelihood term in the definition of Bayes’ posterior. This can be done by raising the likelihood function to a power \(\alpha \in [0,1]\), where \(\alpha=1\) recovers Bayes’ posterior and \(\alpha=0\) corresponds to sticking to the prior. The resulting posterior is therefore:</p> \[\pi_\alpha(\theta\vert x) \propto \pi(\theta) p(x\vert \theta)^\alpha,\] <p>This idea has been discussed in detail in<d-cite key="holmes2017assigning"></d-cite> (and previous papers referred there). The authors of <d-cite key="holmes2017assigning"></d-cite> also propose a way to automatically tune \(\alpha\); this way works by matching the expected information gain in two experiments, one involving the exact data-generating process and the other one involving instead the best model approximation. This strategy recovers \(\alpha=1\) in the well specified case, and sets \(\alpha&lt;1\) otherwise. Other methods for tuning \(\alpha\) are reviewed in <d-cite key="wu2020comparison"></d-cite>.</p> <p>With respect to standard Bayes’ posterior, this strategy does not change the parameter value on which learning is performed, but it only changes the speed of learning (ie the rate of concentration of the posterior distribution with an increasing number of observations). Also, it still satisfies the likelihood principle and coherence.</p> <h2 id="loss-based-bayesian-inference">Loss-based Bayesian inference</h2> <p>A larger leap is taken in <d-cite key="bissiri2016general"></d-cite>. There, the authors replace the likelihood term with the exponential of a loss function:</p> <p>\begin{equation}\label{Eq:bissiri} \pi_{\ell,w}(\theta\vert x) \propto \pi(\theta) \exp{ - w \cdot \ell (\theta,x)}, \end{equation}</p> <p>where \(w\) is a weight which balances the influence of the loss and prior term. With this update, as argued in <d-cite key="bissiri2016general"></d-cite>, you learn about the parameter value minimizing the expected loss:</p> \[\int \ell (\theta,x) g(x) dx,\] <p>where \(g\) is the data-generating process.</p> <p>In <d-cite key="bissiri2016general"></d-cite>, they show how the update rule above (Eq. \eqref{Eq:bissiri}) can be derived axiomatically from the task of learning about the parameter value minimizing the expected loss, by assuming the observed data to be independent from the prior and inference to be <strong>coherent</strong> in the sense defined above (ie invariant to the order in which the observations were obtained).</p> <p>An advantage to this approach is that you do not need to specify a probabilistic model (ie likelihood) here; inference can be performed on “parameter” values that are solely defined through the loss function.</p> <p>Of course, the power likelihood posterior is obtained as a special case by setting \(\ell(\theta, x) = - \log p(x\vert \theta)\), and \(w=\alpha\); further setting \(w=1\) gives back the original posterior.</p> <p>However, with a generic loss, setting the value of \(w\) is an arbitrary choice. The larger \(w\), the faster the concentration of the posterior with the increase of the number of observations. In <d-cite key="bissiri2016general"></d-cite>, some heuristic techniques to tune \(w\) are discussed; in general, however, the choice of \(w\) is a subjective choice of the user, which may be driven by practical and computational reasons as well.</p> <p>Finally, notice that a Bernstein-von Mises result still hold for this more general posterior (of course under some regularity conditions). Some very general and applicable formulations are given in <d-cite key="miller2019asymptotic"></d-cite>.</p> <h3 id="scoring-rules-based-bayesian-inference">Scoring rules-based Bayesian inference</h3> <p>So, the loss-based approach in Eq. \eqref{Eq:bissiri} works without a probabilistic model. But what if you have a misspecified model which you believe carries some meaning about the process you are studying?</p> <p>As mentioned above, using the original Bayes’ posterior may not be a wise choice. In order to perform inference in a sounded way, an idea is to use the loss-based approach and express the loss \(\ell\) as a function of the probabilistic model:</p> \[\ell(\theta, x) = S(p(\cdot|\theta), x),\] <p>where \(S\) is a function of a probability distribution and an observation, which is usually called <em>scoring rule</em> in the literature<d-cite key="gneiting2007strictly"></d-cite>. Therefore, this yields the following update:</p> \[\pi_{S,w}(\theta\vert x) \propto \pi(\theta) \exp\{ - w \cdot S (p(\cdot\vert\theta),x)\}.\] <p>This approach has been investigated in some recent works, amongst which <d-cite key="pacchiardi2021generalized"></d-cite>, <d-cite key="jewson2018principles"></d-cite> , <d-cite key="loaiza2019focused"></d-cite> and <d-cite key="giummole2019objective"></d-cite>. In this way, you learn about the parameter value minimizing the expected scoring rule over the data generating process \(g\):</p> \[\int S (p(\cdot|\theta),x) g(x) dx.\] <p>For some scoring rules \(S\), the above expectation corresponds to a statistical divergence (for instance the energy distance and MMD <d-cite key="pacchiardi2021generalized"></d-cite>). Therefore, you can learn about the parameter value for which the model is closer to the data generating process according to different divergences from the KL one (which, as discussed at the beginning, is what the original Bayes’ posterior leads to). If you are interested for instance in prediction tasks (for which scoring rules are often used as evaluation criteria), this approach is a way to obtain better predictions with a misspecified model in a Bayesian setting<d-cite key="loaiza2019focused"></d-cite>.</p> <p>Clearly, this still gives a coherent update, but the likelihood principle is not satisfied anymore (as we use the likelihood, but do not just evaluate it at the observation); however, the likelihood principle itself does not seem very reasonable if the model is misspecified in the first place<d-cite key="jewson2018principles"></d-cite>.</p> <h2 id="generalized-variational-inference">Generalized Variational Inference</h2> <p>One more step towards generality and we find the approach presented in <d-cite key="knoblauch2019generalized"></d-cite>.</p> <p>The idea is to start from the variational formulation of Bayes’ posterior which is attributed to Donsker and Varadhan <d-cite key="donsker1975asymptotic"></d-cite>; say we have \(n\) independent observations \(\mathbf {x} = (x_1, x_2, \ldots, x_n)\); then, it can be shown that Bayes’ posterior is defined by:</p> \[\pi(\cdot|\mathbf x) = \underset{q \in \mathcal P (\Theta)}{\operatorname{argmin}}\left\{\mathbb{E}_{q(\theta)}\left[-\sum_{i=1}^{n} \log \left(p\left(x_{i} \mid {\theta}\right)\right)\right]+D_{\text{KL}}(q \| \pi)\right\},\] <p>where \(\Theta\) denotes the values that \(\theta\) can take, \(\mathcal P (\Theta)\) is the set of all probability distributions on \(\Theta\), and \(D_{\text{KL}}(q \| \pi)\) is the KL divergence between the distribution \(q\) and the prior \(\pi\).</p> <p>This formulation leads to an <em>optimization-centric</em> view of Bayesian inference, as the authors of <d-cite key="knoblauch2019generalized"></d-cite> put it; additionally, the loss-based posterior in Eq. \eqref{Eq:bissiri} <d-cite key="bissiri2016general"></d-cite> can be obtained in a similar fashion by just replacing the negative log-likelihood with the generic loss function \(\ell\):</p> \[\pi_{\ell,w}(\cdot|\mathbf x) = \underset{q \in \mathcal P (\Theta)}{\operatorname{argmin}}\left\{\mathbb{E}_{q(\theta)}\left[w \sum_{i=1}^{n} \ell(\theta, x_i) \right]+D_{\text{KL}}(q \| \pi)\right\}.\] <p>This formulation open the doors to additional extensions by changing \(D_{\text{KL}}\) in the optimization objective above to a different divergence, say \(D\). Further, you can also restrict the minimization problem to a constrained space of distributions \(\mathcal \Pi \subseteq \mathcal P(\Theta)\). The resulting update rule obtained in this way is termed <em>the Rule of Three (RoT)</em>, as you obtain a posterior distribution by specifying the loss, the space of distributions \(\mathcal \Pi\) and the divergence \(D\):</p> \[\pi_{\ell, D, \mathcal \Pi}(\cdot|\mathbf x) = \underset{q \in \mathcal \Pi }{\operatorname{argmin}}\left\{\mathbb{E}_{q(\theta)}\left[w \sum_{i=1}^{n} \ell(\theta, x_i) \right]+D(q \| \pi)\right\} \stackrel{\text{def}}{=} P(\ell, D, \Pi).\] <p>However, it is in general impossible to obtain a closed form solution for the above problem. Additionally, you do not have a coherent update anymore.</p> <p>In the paper <d-cite key="knoblauch2019generalized"></d-cite>, they show how the above problem generalizes standard Variational Inference (VI, in which you select the distribution in a given class which minimizes the KL divergence from the exact posterior), hence the name of the approach. Therefore, the RoT unifies under the same hat variational inference, loss based posteriors and the standard Bayes’ posterior. It is important to notice how, instead, VI approaches which employ different divergences from the KL (which they call divergence VI) are not included<d-footnote>This means that standard VI (with the KL) is theoretically optimal with respect to divergence VI as the former directly approximates the exact posterior. That however is restricted to the case in which the likelihood and prior are well specified, or to the case in which the variational family $\Pi$ is large enough; if that is not the case, it may be that divergence VI gives better results (as it is sometimes observed in the literature; see Section 2.4 in <d-cite key="knoblauch2019generalized"></d-cite>).</d-footnote>.</p> <p>The rest of the work discusses in very much detail (more than 100 pages!) the properties of the resulting posterior. The key idea is that with this approach you can not only perform inference with a generic loss (as it was already with the loss-based approach by <d-cite key="bissiri2016general"></d-cite> in Eq. \eqref{Eq:bissiri}), but also choose which properties of the prior to consider by picking the right \(D\), and finally reducing the computational burden posed by an unconstrained \(\mathcal P(\Theta)\) by choosing a suitable space of probability distributions \(\Pi\). That therefore tackles the other issues with standard Bayesian inference that I mentioned above.</p> <p>They also provide an axiomatic derivation of the RoT; interestingly, among the required axioms is a generalized likelihood principle, which states that all information on \(\theta\) contained in \(\mathbf x\) is accessible through evaluation of the loss \(\ell\) at the datapoints.</p> <h2 id="recap">Recap</h2> <p>I have gone up the ladder of generality, from standard Bayes’ posterior to the General Variational Inference formulation; at each step, some features of standard Bayes’ posterior are lost and some others are retained, but a larger set of tools become available. Those may work better for instance with misspecified models, or in case you do not even want to specify a full probabilistic model, or finally if a full Bayesian analysis is too costly and you want to instead resort to a variational approach.</p> <p>The following Venn’s diagram represents the relation between the different techniques presented here:</p> <div class="l-body"> <img class="img-fluid" src="/assets/img/blog/2021-08-05-generalizedBayes/venn_diagram.svg"/> </div> <h1 id="conclusion">Conclusion</h1> <p>I have followed here a possible generalization route, but that is by no means the only possibility and my overview does not include all methods which are a superset of standard Bayes’ posterior - I have for instance excluded the approach taken in <d-cite key="fong2021martingale"></d-cite> which puts the focus on the predictive distribution rather than the likelihood, or the one in <d-cite key="masegosa2019learning"></d-cite>, which uses PAC-Bayesian bounds to define distributions which do not concentrate on one single parameter value in the limit of infinite data if the model is misspecified, thus obtaining better predictive performance.</p> <p>Still, I think the approaches reviewed here are thought-provoking and allow to see Bayesian inference under a different light. I also feel they bring the original axiomatic theory closer to a pragmatical toolbox.</p> <p>Of course, I do not think now that standard Bayesian inference has to be thrown to the garbage; there is no need to list the practical results that have been possible with it. Still, it is good to know there are ways around (or beyond) in case a standard Bayesian analysis doesn’t work. Additionally, some of the techniques developed to work with standard posteriors (for instance MCMC and VI) can be ported to generalized posteriors.</p> <p>I am sure there will be other extensions proposed in the future, so keep an eye out if you are interested!</p>]]></content><author><name>Lorenzo Pacchiardi</name></author><summary type="html"><![CDATA[Updating a 250 years old theorem for the 21st century]]></summary></entry><entry><title type="html">Sampling from doubly intractable distributions - the ExchangeMCMC algorithm</title><link href="http://lorenzopacchiardi.me//blog/2020/exchangeMCMC/" rel="alternate" type="text/html" title="Sampling from doubly intractable distributions - the ExchangeMCMC algorithm"/><published>2020-12-29T00:00:00+00:00</published><updated>2020-12-29T00:00:00+00:00</updated><id>http://lorenzopacchiardi.me//blog/2020/exchangeMCMC</id><content type="html" xml:base="http://lorenzopacchiardi.me//blog/2020/exchangeMCMC/"><![CDATA[<h1 id="motivation">Motivation</h1> <p>In Bayesian Statistics, the main object of interest is the posterior distribution of the parameters \(\theta\) given some observed data \(x\). Specifically, given a prior distribution \(\pi(\theta)\) and a likelihood \(p(x\vert \theta)\), the posterior is (via Bayes’ theorem):</p> \[\pi(\theta\vert x) = \frac{\pi(\theta) p(x\vert \theta)}{p(x)}, \qquad p(x) = \int \pi(\theta) p(x\vert \theta) d\theta.\] <p>Markov Chain Monte Carlo (MCMC) techniques are widely used to sample from \(\pi(\theta\vert x)\); these usually require access to \(\pi(\theta)\) and \(p (x\vert\theta)\) but do not require evaluating \(p(x)\) (we show this below for the Metropolis-Hastings algorithm).</p> <p>However, there are cases in which the likelihood function is known only up to a normalizing constant:</p> <p>\(p(x\vert \theta) = \frac {\tilde{p}(x\vert \theta)}{Z(\theta)},\) where \(\tilde p(x\vert \theta)\) is an unnormalized version of the likelihood and \(Z(\theta)\) is intractable. The posterior in this case is said <strong>doubly intractable</strong> as there are two sources of intractability: \(p(x)\) and \(Z(\theta)\).</p> <p>In this case, MCMC cannot be straightforwardly used. Below, we describe a popular extension of Metropolis-Hastings that works in the case of doubly intractable distributions; but first, let us recall the original Metropolis-Hastings algorithm.</p> <h1 id="metropolis-hastings-algorithm">Metropolis-Hastings algorithm</h1> <p>The standard Metropolis-Hastings algorithm uses a distribution \(q ( \cdot \vert \theta)\) to generate a proposal value \(\theta'\) starting from the current state of the chain \(\theta\); then, this proposal is accepted with probability \(\min\{1, \alpha \}\), where:</p> <p>\begin{equation} \alpha = \frac{q(\theta\vert \theta’) \pi(\theta’) p(x\vert \theta’)}{q(\theta’\vert \theta) \pi(\theta) p(x\vert \theta)} = \frac{q(\theta\vert \theta’) \pi(\theta’) \tilde {p}(x\vert \theta’) }{q(\theta’\vert \theta) \pi(\theta) \tilde {p}(x\vert \theta)} \textcolor{red}{\frac{Z(\theta)}{Z(\theta’)}}. \end{equation}</p> <p>As you can see, this does not require knowing \(p(x)\), but instead requires the ratio \({Z(\theta)}/{Z(\theta')}.\) It cannot therefore be applied in the setting of doubly-intractable distributions.</p> <div class="l-body"> <img class="img-fluid" src="/assets/img/blog/2020-12-30-exchangeMCMC/alg1.svg"/> </div> <h1 id="the-exchangemcmc-algorithm">The ExchangeMCMC algorithm</h1> <p>The ExchangeMCMC algorithm (Murray et al., 2012, <d-cite key="murray2012mcmc"></d-cite>) bypasses this issue by drawing an auxiliary observation</p> \[x' \sim p(\cdot\vert \theta')\] <p>and defining the acceptance probability as:</p> <p>\begin{equation}\label{Eq:acc_rate_exchange} \alpha = \frac{q(\theta\vert \theta’) \pi(\theta’)p(x\vert \theta’)}{q(\theta’\vert \theta)\pi(\theta) p(x\vert \theta)}\textcolor{blue}{\frac{p(x’\vert \theta)}{p(x’\vert \theta’)}} = \frac{q(\theta\vert \theta’) \pi(\theta’)\tilde p(x\vert \theta’)\textcolor{blue}{\tilde p(x’\vert \theta)}}{q(\theta’\vert \theta)\pi(\theta)\tilde p(x\vert \theta)\textcolor{blue}{\tilde p(x’\vert \theta’)}} \frac{\cancel{Z(\theta)\textcolor{blue}{Z(\theta’)}}}{\cancel{Z(\theta’)\textcolor{blue}{Z(\theta)}}}, \end{equation} where the black part is the same as in the original Metropolis-Hastings algorithm, but adding the blue terms allow to evaluate \(\alpha\) without estimating the normalizing constants. Still, the resulting algorithm targets the correct distribution.</p> <div class="l-body"> <img class="img-fluid" src="/assets/img/blog/2020-12-30-exchangeMCMC/alg2.svg"/> </div> <h1 id="some-extensions">Some extensions</h1> <h2 id="bridging">Bridging</h2> <p>The acceptance rate in Eq.\eqref{Eq:acc_rate_exchange} depends on two ratios: \(\frac{p(x\vert \theta')}{p(x\vert \theta)}\) represents how well the proposed parameter value explains the observation with respect to the previous parameter value, while \(\frac{p(x'\vert \theta)}{p(x'\vert \theta')}\) measures how well the auxiliary variable (generated using \(\theta'\)) can be explained with parameter \(\theta\).</p> <p>Therefore, even if the former is large and \(\theta'\) would be a suitable parameter value, \(\alpha\) can still be small if the auxiliary random variable is not explained well by the previous parameter value. This can lead to slow mixing of the chain; to improve on this, Murray et al., 2012, proposed to sample a set of auxiliary variables \((x'_0, x'_1, \ldots, x'_K)\) from intermediate distributions in the following way: consider a set of densities: \(\tilde p_k(x \vert \theta, \theta') = \tilde p(x\vert \theta')^{\beta_k} \tilde p(x\vert \theta)^{1- \beta_k}, \quad \beta_k = \frac{K - k + 1}{K + 1};\) \(x'_0\) is generated from \(p(\cdot\vert \theta')\) as before, and then each \(x'_k\) is generated from \(R(\cdot \vert x'_{k-1}; \theta, \theta')\), which denotes a Metropolis-Hastings transition kernel starting from \(x'_{k-1}\) with stationary density \(\tilde p_k(\cdot\vert \theta, \theta')\). Then, the acceptance rate is modified as follows: \begin{equation}\label{Eq:acc_rate_exchange_bridging} \alpha = \frac{\tilde p(x\vert \theta’)q(\theta\vert \theta’) \pi(\theta’)}{\tilde p(x\vert \theta)q(\theta’\vert \theta)\pi(\theta)} \cdot \prod_{k=0}^{K} \frac{\tilde p_{k+1}(x’_k\vert \theta, \theta’) }{\tilde p_k(x’_k\vert \theta, \theta’) }. \end{equation}</p> <p>Note that \(K=0\) recovers the original ExchangeMCMC. This procedure, usually called <em>bridging</em>, generally improves the acceptance rate as it basically introduces a sequence of intermediate updates to the auxiliary data which by smoothening out the difference between the two distributions.</p> <div class="l-body"> <img class="img-fluid" src="/assets/img/blog/2020-12-30-exchangeMCMC/alg3.svg"/> </div> <h2 id="exchangemcmc-without-perfect-simulations">ExchangeMCMC without perfect simulations</h2> <p>If it is not possible to sample from \(p(\cdot\vert \theta')\) as it is required in the ExchangeMCMC algorithm, Murray et al., 2012, suggested to run \(T_{in}\) steps of an MCMC chain on \(x\) targeting \(p(\cdot\vert \theta')\) at each step of ExchangeMCMC; if \(T_{in}\) is large enough, the last sample can be considered as (approximately) drawn from \(p(\cdot\vert \theta')\) itself and used in place of the unavailable perfect simulation.</p> <p>In practice, however, this only leads to an approximate ExchangeMCMC algorithm, as at each iteration of the inner chain a finite \(T_{in}\) is used, so that the inner chain would not perfectly converge to its target; for this reason, even an infinitely long outer chain would not target the right posterior for any finite \(T_{in}\).</p> <p>Nonetheless, this approach was shown empirically to work satisfactorily <d-cite key="liang2010double"></d-cite> <d-cite key="caimo2011bayesian"></d-cite> <d-cite key="everitt2012bayesian"></d-cite>; moreover, it has been argued <d-cite key="liang2010double"></d-cite> that starting the inner chain from the observation value improves convergence.</p> <p>Some theoretical guarantees (albeit under strongs conditions), are given in Appendix B of Everitt, 2012 <d-cite key="everitt2012bayesian"></d-cite>, which bounds the total variation distance between the target of approximate ExchangeMCMC with finite \(T_{in}\) and the target of the exact one, and shows that they become equal when \(T_{in} \to \infty\).</p> <div class="l-body"> <img class="img-fluid" src="/assets/img/blog/2020-12-30-exchangeMCMC/alg4.svg"/> </div> <h1 id="related-algorithms">Related algorithms</h1> <p>ExchangeMCMC algorithm is very popular due to its ease of use. However, it is certainly not the only algorithm to sample from doubly intractable distribution (see the nice review given in Park and Haran, 2018 <d-cite key="park2018bayesian"></d-cite>); some are listed next:</p> <ul> <li>Caimo et al., 2011 <d-cite key="caimo2011bayesian"></d-cite> proposed a parallel-chain MCMC algorithm</li> <li>Everitt et al., 2012<d-cite key="everitt2017marginal"></d-cite> built an SMC-type algorithm which is also capable of recycling information from past simulations</li> <li>Finally, Liang et al., 2016 <d-cite key="liang2016adaptive"></d-cite> proposed an algorithm which is inspired from ExchangeMCMC and, in the case of impossible perfect sampling, still targets the right invariant distribution. This algorithm requires some hand-tuning and needs to keep in memory a large amount of data, which may hinder its applicability.</li> </ul> <p>Differently from ExchangeMCMC, the above ones can be parallelized.</p>]]></content><author><name>Lorenzo Pacchiardi</name></author><summary type="html"><![CDATA[A quick review]]></summary></entry></feed>