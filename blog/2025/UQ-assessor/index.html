<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Predicting when LLMs fail | Lorenzo Pacchiardi </title> <meta name="author" content="Lorenzo Pacchiardi"> <meta name="description" content="An overview of different paradigms to do so and connections with related areas"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="http://lorenzopacchiardi.me//blog/2025/UQ-assessor/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Predicting when LLMs fail",
            "description": "An overview of different paradigms to do so and connections with related areas",
            "published": "January 03, 2025",
            "authors": [
              
              {
                "author": "Lorenzo Pacchiardi",
                "authorURL": "uq_assessors",
                "affiliations": [
                  {
                    "name": "University of Cambridge",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lorenzo </span> Pacchiardi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">resources </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Predicting when LLMs fail</h1> <p>An overview of different paradigms to do so and connections with related areas</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#intro">Intro</a> </div> <div> <a href="#paradigms-to-predict-where-llms-fail">Paradigms to predict where LLMs fail</a> </div> <ul> <li> <a href="#intrinsic-uncertainty-quantification">Intrinsic uncertainty quantification</a> </li> <li> <a href="#assessors-anticipating-llm-performance">Assessors - anticipating LLM performance</a> </li> <li> <a href="#model-agnostic-rejectors">Model-agnostic rejectors</a> </li> <li> <a href="#bonus-modifying-the-llm-itself">Bonus - Modifying the LLM Itself</a> </li> </ul> <div> <a href="#tangential-directions">Tangential directions</a> </div> <ul> <li> <a href="#extracting-other-information-from-model-internals">Extracting other information from model internals</a> </li> <li> <a href="#using-llms-for-forecasting">Using LLMs for forecasting</a> </li> <li> <a href="#reward-models">Reward models</a> </li> <li> <a href="#how-humans-interpret-llm-uncertainty">How humans interpret LLM uncertainty</a> </li> <li> <a href="#robustness-of-llms-to-input-perturbation">Robustness of LLMs to input perturbation</a> </li> </ul> <div> <a href="#further-reading">Further reading</a> </div> </nav> </d-contents> <p><em>Updated on 30th Jan 2025 by adding three papers</em></p> <h1 id="intro">Intro</h1> <p>Large Language Models (LLMs) are ML systems. As for any ML system, we may want to predict whether they will fail or succeed on specific task instances (e.g., a specific question from a QA dataset).</p> <p>There are several interrelated paradigms to do so, differing in whether they are specific to a model or not and whether they rely on the model’s outputs or internals. The aim of this post is to frame how these different approaches relate to one another. Therefore, I overview below three main paradigms, and point towards closely related research fields that tackle different questions.</p> <p><em>Disclaimer:</em> <em>I do not claim this overview to cover all existing research strands related to predicting LLM failures or to comprehensively capture all works in each of the considered ones. I welcome any feedback.</em></p> <p>TL;DR: in brief, I identified three paradigms that can predict where LLMs fail. The third one is likely very hard to apply to LLMs in practice. The table below summarises their properties:</p> <ul> <li>whether additional training is required</li> <li>whether the learned approach is model-specific</li> <li>whether the input must be passed through the model to predict whether the latter will fail or not</li> </ul> <table border="1"> <thead> <tr> <th>CATEGORY</th> <th>TRAINING</th> <th>MODEL-SPECIFIC</th> <th>PASSING INPUT THROUGH THE MODEL</th> </tr> </thead> <tbody> <tr> <td>Intrinsic UQ</td> <td>In some cases (eg whitebox methods)</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>Assessor</td> <td>Yes</td> <td>Yes (they can be trained to work for a set of models, based on model features alongside instance features)</td> <td>No</td> </tr> <tr> <td>Model-agnostic rejector</td> <td>Yes, but once per data distribution</td> <td>No</td> <td>No</td> </tr> </tbody> </table> <h1 id="paradigms-to-predict-where-llms-fail">Paradigms to predict where LLMs fail</h1> <h2 id="intrinsic-uncertainty-quantification">Intrinsic uncertainty quantification</h2> <p>Uncertainty quantification (UQ) has been extensively researched in “traditional” deep learning. It involves a suite of techniques aimed at extracting an uncertainty quantification from a trained model (such as reporting the logits of a trained multi-class classifier network) or training those models to embed a form of uncertainty (such as Bayesian Neural Networks, or BNNs). Most methods falling under the term “uncertainty quantification” in the traditional deep learning literature involve producing an output with the model and accompanying it with an uncertainty estimate obtained in some way.</p> <p>Some of these methods have been adapted or evolved into approaches applicable to LLMs; however, other traditional methods are hardly applicable to them. In fact, with respect to traditional deep learning, LLMs have two main differences that limit the applicability of existing methods and spurred the creation of novel ones:</p> <ul> <li>first, training a LLM is extremely costly. As such, it is hard to apply uncertainty quantification methods that require bespoke training approaches (such as BNNs).</li> <li>second, LLMs do not perform regression or classification, but rather language generation in an auto-regressive manner: to answer a specific question, a LLM recursively generates new tokens to produce a suitable completion. Even if token-level logits (and thus probabilities) are immediately available, it is not trivial to obtain an answer-level uncertainty estimate from them. This therefore spurred the creation of novel approaches specific to LLMs.</li> </ul> <p>Shorinwa et al., 2024, <d-cite key="shorinwa2024survey"></d-cite> provides a nice overview of existing UQ methods for LLMs and their connection with classical ones. Beyond token-level uncertainty quantification, a second class of methods involve having the models verbalise their uncertainty, either after or before an answer has been produced <d-cite key="kadavath2022language"></d-cite><d-cite key="lin2022teaching"></d-cite><d-cite key="kapoor2024large"></d-cite>. A third category is that of “semantic similarity” approaches; for instance, Kuhn et al., 2023, <d-cite key="kuhn2023semantic"></d-cite> generates different answers, clusters them according to meaning and considers the uncertainty over clusters. Finally, a set of “white-box” works rely on training additional modules that, starting from model activations at a given internal layer, predicts the probability of a produced answer being correct <d-cite key="kadavath2022language"></d-cite><d-cite key="ferrando2024entity"></d-cite>. To extend the applicability of those methods to black-box LLMs, a recent work <d-cite key="sam2025predicting"></d-cite> ask a set of “elicitation questions” after an answer has been generated and uses the binary responses (or log probs, if available) to predict whether the answer was correct, thus replacing the representation obtained from model internals with the responses to those questions.</p> <p>In general, there does not seem to be complete consensus on whether uncertainty quantification methods for LLMs work well. See Sections 1 and 2 of <d-cite key="kapoor2024large"></d-cite> for a nice overview. For instance, a recent work <d-cite key="pawitan2024confidence"></d-cite> has shown how the model confidence obtained by those different methods is not necessarily consistent: indeed, they find that the token-level probabilities and verbalised confidence are very poorly correlated.</p> <p>The approaches above differ in terms of level access: from access to activations (white-box), to log-probabilities (token-level approaches), to necessity of multiple generation (as for some semantic similarity approaches), to simple generation for verbalised UQ. Across all of those categories, some works involve finetuning the model to improve its UQ ability <d-cite key="kadavath2022language"></d-cite>.</p> <p>Nevertheless, all UQ approaches for LLMs require to produce model outputs (or even multiple versions), or at least to pass the input through the LLM to obtain its activations.</p> <h2 id="assessors-anticipating-llm-performance">Assessors: anticipating LLM performance</h2> <p>An alternative paradigm which has received less attention is that of “assessors” <d-cite key="hernandez2022training"></d-cite>: additional modules that are trained to predict the correctness of a main model on specific instances <strong>without</strong> relying on that model’s output or activations <d-footnote>The original nomenclature also included assessors relying on LLM output, which would make the “white-box” unceratinty quantification methods also fall under this. For ease of understanding, I will then consider all assessors to not rely on LLM outputs</d-footnote>. In practice, assessors are classifiers that tackle the problem of predicting the success of the main model starting from a set of features of the input <d-cite key="schellaert2024analysing"></d-cite><d-cite key="zhou2022reject"></d-cite>, such as some embeddings. This makes the assessor model-specific. Alternatively, a single assessor can be trained to predict the success of multiple models, by relying on specific features to identify them <d-cite key="pacchiardi2024instances"></d-cite>, thus predicting success of a model on a specific instance from a pair (instance features, model features).</p> <p>In both cases, the main point is that the model does not require to see the input at test time for the assessor to make a prediction of its performance. This has two advantages:</p> <ul> <li>first, it reduces the computational cost of running an input through an LLM if it will predictably fail.</li> <li>second, it makes the assessor robust to manipulation: if an LLM receives feedback from an assessor that uses its outputs, the LLM could learn to produce manipulative output, similarly to what happens with RLHF <d-cite key="wen2024mislead"></d-cite>.</li> </ul> <p>The concept of assessors is closely related to that of routers <d-cite key="hu2024routerbench"></d-cite>, which are modules that predict which of a set of LLMs is more likely to succeed at a specific task instance.</p> <h2 id="model-agnostic-rejectors">Model-agnostic rejectors</h2> <p>Model-agnostic rejectors operate on the assumption that a model is likely to fail when confronted with a data point significantly different from the training distribution. This concept is closely aligned with anomaly detection. To implement this, a predictor is trained on the training data to identify inputs that diverge notably from that distribution.</p> <p>I am unaware of any work applying this idea to LLMs; I believe the main reason is that LLMs’ training datasets are vast and often unknown, so it is basically impossible to train a rejector using information on that. Even more importantly, as LLMs can tackle a wide range of tasks, it would not make sense to train a rejector on the pre-training dataset.</p> <p>Therefore, the practical usefulness of this approach may be limited to cases where LLMs undergo fine-tuning for specific use cases in niche domains. Even here, however, it may be better to rely on assessors or uncertainty quantification methods which, by exploiting information on the model, are likely to perform better. In fact, the main advantage of model-agnostic rejectors is that they can be applied to any model trained on a specific dataset; however, it is rare to have multiple LLMs trained (or even finetuned) on the same dataset, due to their large cost.</p> <h2 id="bonus-modifying-the-llm-itself">Bonus: Modifying the LLM Itself</h2> <p>A recent study <d-cite key="cohen2024idk"></d-cite> introduced an innovative approach by integrating an “I don’t know” token into the LLM’s vocabulary during the pre-training phase. This addition improved the model’s calibration by enabling it to explicitly express uncertainty.</p> <p>Such advancements echo techniques in uncertainty quantification (UQ) for traditional deep learning, where models are modified or trained in bespoke ways to better capture uncertainty.</p> <h1 id="tangential-directions">Tangential directions</h1> <h2 id="extracting-other-information-from-model-internals">Extracting other information from model internals</h2> <p>There are other works investigating the use of model internals to extract additional insights. For instance, some studies <d-cite key="burger2024truth"></d-cite><d-cite key="azaria2023internal"></d-cite> focus on determining whether a model is engaging in deceptive behavior (commonly referred to as “lying”).</p> <p>This approach bears similarities to extracting uncertainty quantification for LLMs but instead targets different “functions” of a response beyond mere correctness. It could also prove valuable in identifying instances of “sandbagging” <d-cite key="weij2024sandbagging"></d-cite>, where models strategically underperform under specific conditions.</p> <h2 id="using-llms-for-forecasting">Using LLMs for forecasting</h2> <p>Rather than focusing on quantifying uncertainty in LLM-generated answers, this approach leverages LLMs to provide uncertainty quantification for external world events. This is conceptually akin to applying intrinsic model uncertainty to simple QA quizzes but extended to inherently uncertain questions. Instead of addressing questions with definitive answers, this method deals with questions characterized by intrinsic unpredictability. Notable efforts in this domain include:</p> <ul> <li>Dataset: The Autocast dataset <d-cite key="zou2022forecasting"></d-cite> compiles a collection of questions and aggregated human forecasts derived from forecasting tournaments, complemented by a news corpus. In these tournaments, forecasters tackle True/False, multiple-choice, and numerical questions. After the resolution of each question, forecasters are scored using Scoring Rules based on the forecasts they provided at earlier dates. An LLM can simulate forecasting for a specific date by processing news articles up to that date and attempting to answer the questions. The diverse question formats and the extensive numerical ranges make this a challenging benchmark. Consequently, Autocast serves as an outstanding real-world testbed for developing scalable uncertainty quantification methods tailored to LLMs.</li> <li>Experiments: Recent studies <d-cite key="schoenegger2024wisdom"></d-cite><d-cite key="halawi2024forecasting"></d-cite> have explored the use of agentic architectures, enabling LLMs to actively engage in forecasting tasks. These experiments push the boundaries of LLM capabilities in addressing uncertainty in dynamic, real-world scenarios.</li> </ul> <h2 id="reward-models">Reward models</h2> <p>Reward models can be considered as somehow related to uncertainty quantification: they are typically trained (for instance from human feedback) to evaluate a completion in terms of a validity specification. However, this validity specification usually includes absence of toxic content or alignment with user intentions, rather than correctness of an answer, even if the latter could plausibly be used. Then, they are used as the reward function to finetune LLMs via RL. However, they can also be independently used to evaluate completions produced by a model; some works also show that they can be used to evaluate partial completions, thus stopping generation and then restarting it if a generation is likely to be unsuccessful wrt the final specification <d-cite key="nath2024goal"></d-cite>.</p> <p>However, in contrast to intrinsic uncertainty quantification and assessors, reward models are generally not specific to a single LLM. As such, they rely on generic features of the output that can be used to understand agreement with the considered validity specification. In contrast, assessors and uncertainty quantification approaches are trained considering a specific model or a set of models and explicitly or implicitly rely on their features.</p> <h2 id="how-humans-interpret-llm-uncertainty">How humans interpret LLM uncertainty</h2> <p>Human perception and prediction of LLM performance represents another important research direction. Studies have revealed significant limitations in human ability to anticipate LLM behavior. A recent experiment <d-cite key="carlini2024gpt"></d-cite> has shown that humans perform only marginally better than random chance when predicting GPT-4’s performance. Moreover, humans tend to exhibit overconfidence in predicting LLM performance on high-stakes tasks, likely due to anchoring on past interactions with these systems <d-cite key="vafa2024large"></d-cite>. A particularly concerning finding is that LLM success on challenging tasks within a domain does not necessarily translate to reliable performance on simpler tasks in the same domain <d-cite key="zhou2024larger"></d-cite>. This counterintuitive behavior challenges our natural assumptions about machine learning systems and highlights the importance of systematic evaluation approaches. Additionally, a recent work <d-cite key="steyvers2025large"></d-cite> shows that, even for LLMs that are well-calibrated in terms of the probabilities they assign to the tokens indicating the options in multiple-choice scenarios, the explanations they provide to justify the chosen option lead humans to become overconfident in whether the LLM is correct (the authors term this the “calibration gap”). Moreover, the authors show that longer LLM explanations increase the confidence of users even if accuracy does not improve. However, they find that it is possible to adjust LLM explanations to better reflect the models internal confidence, which allows to reduce the calibration gap. Another work <d-cite key="kapoor2024large"></d-cite> subject humans to a test with the assistance of an LLM with several uncertainty quantification methods (more or less calibrated) and find that users modulate their choices following the confidence outputs, indicating that improving calibration of those confidences is important.º</p> <h2 id="robustness-of-llms-to-input-perturbation">Robustness of LLMs to input perturbation</h2> <p>The sensitivity of LLMs to minor prompt modifications has been well-documented in the literature <d-cite key="dhole2022augmenter"></d-cite><d-cite key="shen2024jailbreak"></d-cite>. Research has demonstrated that even subtle changes to input prompts can result in substantially different outputs, raising questions about the reliability and consistency of these systems. This sensitivity to input perturbations suggests a potential avenue for uncertainty estimation: by measuring a model’s response variability to small input modifications, we could potentially identify cases where the model exhibits high uncertainty and should be rejected. While this approach has been theoretically proposed in recent literature reviews, particularly in Hendrickx et al.’s 2024 review <d-cite key="hendrickx2024reject"></d-cite>, its practical application to LLMs remains unexplored.</p> <h1 id="further-reading">Further reading</h1> <ul> <li>A comprehensive introduction to uncertainty quantification in NLP can be found in a <a href="https://sites.google.com/view/uncertainty-nlp" rel="external nofollow noopener" target="_blank">2022 tutorial</a> that covers fundamental concepts and key techniques in the field. While slightly dated, it provides valuable foundational knowledge.</li> <li>Hendrickx et al.’s 2024 review <d-cite key="hendrickx2024reject"></d-cite> on machine learning with reject option offers broader insights beyond the LLM context. The review introduces a useful taxonomy, distinguishing between novelty rejection (for inputs substantially different from training data) and ambiguity rejection (for cases where training data or learned algorithms show uncertainty). It proposes three architectural principles: <ul> <li>Separate rejectors operate independently of any predictor, aligning with the model-agnostic rejector paradigm discussed earlier. Notably, while assessors don’t rely on model outputs, they differ from separate rejectors as they incorporate information about predictor performance.</li> <li>Dependent rejectors utilize predictor outputs, often through confidence functions or input perturbation techniques, encompassing many of the UQ approaches outlined above.</li> <li>Integrated rejectors combine rejection capability within the primary model, typically by introducing an additional “reject” class. This category includes both certain UQ methods and approaches involving LLM modification during training.</li> </ul> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-UQ_assessor.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lorenzo Pacchiardi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: September 17, 2025. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>